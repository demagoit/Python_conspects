M1 22.01.24

SOLID - це важливо, 100% запитають на інтервью
автотести на pet-проектах - це важливий + у карму


M1 29.01.24
video 0:30 [scripts] - точка входу, запуск скрипта pipenv run my_app

M2
Python має пакет, який фактично став золотим стандартом — це black.
black суворо дотримується PEP8, але вносить низку своїх власних правил, які покращують сприйняття коду. За потреби, можна налаштувати black "під себе", але в 99% випадків стандартних налаштувань цілком достатньо.
PyCharm IDE: https://plugins.jetbrains.com/plugin/14321-blackconnect
VSCode: https://marketplace.visualstudio.com/items?itemName=ms-python.black-formatter

Статичні аналізатори, що найчастіше використовуються:
Pylint Flake8 MyPy Radon

Оскільки такі об'єкти, як списки, словники та кортежі містять інші об'єкти, іноді нам потрібно ввести підказки, щоб зазначити, які типи об'єктів вони містять. Для цього нам потрібно звернутись до модуля typing, який надає інструменти для опису типів.
from typing import List
Data = List[float | int]

Надаю вам те, що вам дуже допоможе:wink:
Шпаргалки по Docker
Робота з реєстрами та репозиторіями Docker:
- Вхід у реєстр: `docker login` або `docker login localhost:8080`
- Вихід з реєстру: `docker logout` або `docker logout localhost:8080`
- Пошук образу: `docker search nginx`
- Pull образа: `docker pull nginx`
- Push образа: `docker push eon01/nginx`
Початкові кроки з контейнерами:
- Створення контейнера: `docker create -t -i eon01/infinite --name infinite`
- Перший запуск контейнера: `docker run -it --name infinite -d eon01/infinite`
- Перейменування контейнера: `docker rename infinite infinity`
- Видалення контейнера: `docker rm infinite`
- Оновлення контейнера: `docker update --cpu-shares 512 -m 300M infinite`
Запуск та зупинка контейнерів:
- Запуск зупиненого контейнера: `docker start nginx`
- Остановка: `docker stop nginx`
- Перезавантаження: `docker restart nginx`
- Пауза: `docker pause nginx`
- Скасування паузи: `docker unpause nginx`
- Блокування: `docker wait nginx`
- Відправка сигналу SIGKILL: `docker kill nginx`
- Підключення до контейнера: `docker attach nginx`
Отримання інформації про контейнери:
- Перегляд запущених контейнерів: `docker ps`
- Логи контейнера: `docker logs infinite`
- Інформація про контейнер: `docker inspect infinite`
- Події контейнера: `docker events infinite`
- Публічні порти: `docker port infinite`
- Процеси у контейнері: `docker top infinite`
- Ресурси контейнера: `docker stats infinite`
- Зміни в ФС контейнера: `docker diff infinite`
Управління образами:
- Список образів: `docker images`
- Створення образа: `docker build .`
- Видалення образа: `docker rmi nginx`
- Завантаження образа: `docker load < ubuntu.tar.gz`
- Збереження образа: `docker save busybox > ubuntu.tar`
- Історія образа: `docker history`
- Створення образа з контейнера: `docker commit nginx`
- Тегування образа: `docker tag nginx eon01/nginx`
- Push образа: `docker push eon01/nginx`
Мережа Docker:
- Створення мережі: `docker network create -d overlay MyOverlayNetwork`
- Видалення мережі: `docker network rm MyOverlayNetwork`
- Перегляд мереж: `docker network ls`
- Інформація про мережу: `docker network inspect MyOverlayNetwork`
- Підключення контейнера до мережі: `docker network connect MyOverlayNetwork nginx`
- Відключення контейнера від мережі: `docker network disconnect MyOverlayNetwork nginx`
Очищення Docker:
- Видалення контейнера: `docker rm nginx`
- Видалення контейнера та його тома: `docker rm -v nginx`
Docker зберігає кеш для прискорення процесу збірки образів. Видаліть його так:docker system prune -a
Шпаргалки для роботи з GitHub
“echo “# назва” >> README.md” - створює файл README.md
“git init” - ініціалізація репозиторію
“git add README.md” - додавання файлу README.md до проекту
“git commit -m “first commit”" - фіксація змін у репозиторії з певним повідомленням
“git remote add origin https://github.com/stanruss/название.git” - встановлення з‘єднання з віддаленим репозиторієм
“git push -u origin master” - відправлення змін на віддалений сервер
“git log --oneline” - перегляд усіх комітів
“git checkout .” - відновлення всього
“git checkout “код коммита“” - повернення до стану певного коміту
“git checkout master” - повернення до гілки master
Для відновлення файлів на локальному комп’ютері:
```git fetch --all
git reset --hard origin/master``` - скидання змін до останньої версії на сервері
“git add text.txt” - додавання файлу до репозиторію
“git rm text.txt” - видалення файлу
“git status” - поточний стан репозиторію
“git commit -a -m “Commit description”" - створення коміту з усіма змінами
“git push origin” - синхронізація всіх гілок локального репозиторію з віддаленим
“git push origin master” - синхронізація гілки master з віддаленим репозиторієм
“git push origin HEAD” - відправлення поточної гілки на сервер
“git pull origin” - синхронізація всіх гілок з сервером
“git pull origin master” - синхронізація гілки master з сервером
“git fetch origin” - завантаження всіх гілок з серверу без злиття з локальними
“git merge some_branch” - злиття гілки some_branch з поточною
“git branch -d some_branch” - видалення гілки (після злиття)
“git branch -D some_branch” - примусове видалення гілки
“git show <commit-hash>” - перегляд змін в певному коміті
“git push origin :branch-name” - видалення гілки з серверу
“git reset --hard <commit-hash>” - повернення до певного коміту і видалення наступних змін
“git push -f” - примусове відправлення змін на сервер
“git clean -f” - видалення неіндексованих файлів (edited) 
Docker зберігає кеш для прискорення процесу збірки образів, але це займає місце. Видаліть його так: docker system prune -a

M3 
Процес — область пам'яті (віртуальна) + набір ресурсів + 1 і більше потоків.
Потік — послідовність інструкцій та системних викликів всередині процесу.

Завдання, які виконують операції введення/виведення (читання/запис файлів, запити в мережі тощо), називаються IO (Input Output)-bound завданнями. Домогтися паралелізму виконання IO завдань у Python можна, використовуючи потоки.
Інший тип блокуючих викликів — це важкі з точки зору обчислень операції. 
Такі завдання називаються CPU-bound завданнями. Як і для IO-bound завдань, можна винести виконання блокуючих операцій (складних обчислень) в окремий потік, щоб застосунок продовжував взаємодіяти з користувачем, здійснюючи обчислення.

Однак потрібно пам'ятати, що асинхронний код завжди на порядок складніший для розуміння та відлагодження.
Загальне правило для програмування будь-якою мовою: якщо є можливість обійтися синхронним кодом, то так і потрібно зробити.
https://medium.com/swift-india/concurrency-parallelism-threads-processes-async-and-sync-related-39fd951bc61d

Потоки можуть виконуватися дійсно паралельно (якщо ядер процесора більше 1), процеси — тим більше. Але у Python є механізм, який примусово блокує виконання коду різними потоками одного Python процесу в один і той самий час.
Як обійти GIL:
1. Написати частину коду, яку потрібно запускати паралельно, на Cython і використовувати потоки.
2. Використовувати Multiprocessing.

https://docs.python.org/3/library/concurrency.html

Щоб створити потік, найпростіше імпортувати клас Thread з модуля threading і наслідуватись від цього класу. Далі вам потрібно визначити метод run у вашого класу, цей метод буде виконуватись в окремому потоці. Щоб розпочати виконання коду в окремому потоці, потрібно викликати метод start, який визначений у Thread.
Є інший спосіб виконати код окремого потоку. Для цього потрібно, щоб код виконання був функтором (функцією або класом, який має метод __call__). Тоді об'єкт можна передати як іменований аргумент target у Thread.
Або у процесі створення екземпляра класу Thread можна передати аргументу target функцію та передати їй аргументи як кортеж args у Thread.
Основний потік спочатку запискає дочірні потокі, робить свій вивід і закінчується, а після нього дочірні потоки MyThread виводять своє повідомленя, і тільки після цього скрипт завершився.
Коли потрібно в основному застосунку дочекатися виконання потоку, можна скористатися блокуючим методом join.
Ви також можете перевірити — чи виконується потік, викликавши метод is_alive.
Екземпляри класу Timer починають працювати з деякою затримкою, яку визначає програміст. Крім того, ці потоки можна скасувати будь-якої миті методом .cancel() в період затримки.
Оскільки ОС може на будь-якому виклику перервати виконання потоку та передати контроль іншому потоку, ви не можете бути впевненим, що робота із загальним ресурсом буде коректно завершеною і ресурс не опиниться в невизначеному стані.
Для цього є механізм блокування. У Python є два примітива блокувань: Lock та RLock. Lock трохи швидший і більш низькорівневий, але він не рекурсивний і може бути ситуація потрапляння в DeadLock, коли виконання коду заблокується, кілька потоків чекатимуть, доки хтось віддасть Lock, а його ніхто ніколи вже не віддасть. RLock трохи повільніший, зате виключає взаємне блокування (дозволяя одному потоку мати доступ до ресурсів, які він сам заблокував і ще не розблокував), але розблокувати його треба стільки ж раз, скільки було заблоковано.
Блокування ресурсу досягається виконанням команди lock.acquire(), відпускає lock.release()
Але найчастіше для блокування використовують контекст виконання with lock.
Другий примітив синхронізації — це семафори.
Семафори підходять до блокування іншим шляхом та вказують, що кілька потоків можуть користуватися ресурсом одночасно і цим обмежують кількість потоків. Наприклад, ми не хочемо надсилати десятки тисяч запитів до мережі одночасно, щоб не створювати навантаження на обладнання і вкажемо семафор, щоб не більше ста потоків могли одночасно надсилати запити.
Наступним кроком ми розглянемо синхронізацію роботи потоків за допомогою умов та подій.
Є примітиви синхронізації, які дозволяють потокам очікувати сигнал від інших потоків — це Condition.
Методи керування: condition.wait(), condition.notify_all(), condition.notify()
Інший примітив синхронізації — це потокобезпечний прапорець класу Event.
event.set(), event.wait(), event.clear(), event.is_set()
Виникає закономірне питання, навіщо, якщо результат той самий? Справа в тому, що ми можемо керувати виконанням, перезапуском та зупинкою роботи потоків через клас Event. Наприклад, ми перериваємо виконання потоку, який працює в нескінченному циклі та інакше просто ніколи не завершиться, перевіркою break if event_for_exit.is_set().
Останній примітив синхронізації, який ми розглянемо в Python — це бар'єр Barrier.Він дозволяє задати умову, щоб кілька потоків продовжили роботу лише після того, як задане число потоків добереться у виконанні коду до цього "бар'єру". 
Потік може дістатися бар'єру і чекати його за допомогою функції wait(). Це блокуючий виклик, який повернеться, коли решта потоків (попередньо налаштована кількість barrier = Barrier(5)) дістануться бар'єру.
Функція очікування wait() повертає ціле число, яке вказує на кількість учасників, яких ще треба дочекатися. Якщо потік був останнім, що прибув, то повернене значення буде нульовим.
Головна вимога, щоб кількість потоків, що запускаються була кратною кількості бар'єру.
В Python існує ще один механізм написання асинхронного коду. Ви можете скористатися пакетом concurrent.futures. Він дозволяє піднятися на вищий рівень абстракції, коли вам просто потрібно паралельно виконати ряд однотипних завдань і немає необхідності вдаватися до низькорівневих деталей реалізації.
Основна ідея полягає у використанні реалізації абстрактного класу Executor. У concurrent.futures є дві реалізації цього абстрактного базового класу: ProcessPoolExecutor — для виконання коду окремих процесів (з ним ми познайомимося пізніше) та ThreadPoolExecutor — для виконання в окремих потоках.
Кожен такий Executor приховує набір потоків або процесів, яким ви можете дати роботу та отримати результат її виконання. Вам не потрібно вручну управляти створенням потоків та їх коректним завершенням.
Звичайно, все ще потрібно пам'ятати про доступ до загальних ресурсів та примітиви синхронізації.

Пакет multiprocessing — це пакет для виконання коду в окремих процесах з інтерфейсом подібним до інтерфейсу пакета threading.
Основна причина появи multiprocessing — це GIL (Global Interpreter Lock) і той факт, що threading API не дозволяє розпаралелювати CPU-bound завдання. Оскільки в один момент часу завжди виконується код тільки в одному потоці, навіть на багатоядерних сучасних процесорах, отримати приріст продуктивності для завдань, пов'язаних з інтенсивними обчисленнями, за допомогою threading не вийде.
Для використання процесів необхідно імпортувати клас Process модуля multiprocessing. З ним можна працювати декількома способами:
1.У процесі створення екземпляра класу Process іменованому аргументу target передати функцію, яка буде виконуватися в окремому процесі
2.Реалізувати похідний клас від класу Process та перевизначити метод run
https://docs.python.org/3.8/library/multiprocessing.html#programming-guidelines
Залежно від платформи multiprocessing підтримує 3 способи створення нового процесу:
1.spawn — запускає новий процес Python, наслідуються лише ресурси, необхідні для запуску run(). Присутній в Unix і Windows. Є способом за замовчуванням для Windows і macOS.
2.fork — дочірній процес, що є точною копією батьківського (включаючи всі потоки), доступний тільки на Unix. За замовчуванням використовується на Unix. Зробити безпечний fork досить проблематично і це може бути причиною неочевидних проблем.
3.forkserver — створюється процес-фабрика (сервер для породження процесів за запитом). Наслідуються тільки необхідні ресурси, що використовуються fork для запуску нового процесу, але завдяки однопотоковій реалізації процесу-фабрики, це робиться безпечно. Доступний тільки на Unix платформах з підтримкою передачі файлових дескрипторів через pipes (що може суперечити безпековій політиці на багатьох системах).
Для вибору методу використовується multiprocessing.set_start_method(method)
Для міжпроцесорної взаємодії виокремлюють наступні інструменти:
файли;
сокети;
канали (всі POSIX ОС);
роздільна пам'ять (всі POSIX ОС);
семафори (всі POSIX ОС);
сигнали або переривання (крім Windows);
семафори (всі POSIX ОС);
черга повідомлень;
У будь-якому разі, крім загальної пам'яті, для обміну даними між процесами всі об'єкти серіалізуються та десеріалізуються. Цей додатковий крок створює навантаження на CPU.


Пул процесів з multiprocessing дає більше контролю, ніж пул з concurrent.futures.
Основні можливості:
розбиває вхідну послідовність на блоки та виконує паралельну обробку поблоково, так можна зменшити обсяг використовуваної пам'яті;
асинхронне виконання трохи прискорює отримання результатів, якщо порядок не важливий;
передача кортежу аргументів у target-функцію;

Пакет concurrent.futures також реалізує API Executor для пулу процесів у класі ProcessPoolExecutor.
Основні можливості обмежені API Executor. Зручно використовувати ProcessPoolExecutor там, де потрібно виконати CPU-bound завдання в async коді та реалізовано саме для підтримки виконання блокуючих CPU-bound завдань в async застосунках.

video 05.02.24

1:15 - daemon=TRUE вбиває всі потоки, якщо закінчився основний

video 07.02.24
IO-bound задачи нормально оптимізує навіть Python-scheduler (multithread на одному GIL)
CPU-bound задачи Python-scheduler оптимізує погано (треба multiprocess на різних GIL)
в multiprocess є різниця при використанні root-loger та custom-logger

0:38 для widows доступен лише spawn, fork та forkserver недоступні

2:00 у pipe child процеси є копіями головного процесу, тому мають доступ до всіх globals, в т.ч. до всіх оголошених пайпів

2:25 queue.task_done() треба робити після завершення кожної таски в процессі, а не по завершенню процессу, інакше зависне в головному процесі на queue.join() бо  ніколи не отримає підтвердження виконання всіх тасков.

M4
У моделі OSI мережеві функції розподілені між сімома рівнями. Кожному рівню відповідають різні мережеві операції, обладнання та протоколи.
https://textbook.edu.goit.global/python-web-textbook/uk/docs/additional/web/osi
HTTP є синхронним протоколом. Це означає, що клієнт надіслав запит серверу і поки чекає від нього відповідь, наступні запити надіслати не може. Також варто відзначити, що HTTP — протокол без стану. Тобто сервер не зберігає інформацію про користувача між запитами.
Для збереження стану користувача в системі використовується механізм cookie або сесій.
Існує наступна традиційна форма запису `URL:
<scheme>://[<login>[:<password>]@]<host>[:<port>]][/<path>][?<query>][#<fragment>]
Коди стану відповідей сервера
https://en.wikipedia.org/wiki/List_of_HTTP_status_codes
https://docs.python.org/3.10/library/http.html?highlight=http
У Python для роботи з протоколом HTTP можна скористатися пакетом http, який реалізує дуже простий HTTP-сервер.Його не варто використовувати для "бойових" застосунків, але для навчання та розуміння основних механік роботи вебзастосунку його цілком достатньо.
https://docs.python.org/3.9/library/http.server.html
для верстки простіше використовувати готові інструменти на кшталт Bootstrap — вільний набір інструментів для створення сайтів та вебзастосунків. 
https://getbootstrap.com/
Створення чистих HTML-документів не є частим завданням у Python. Набагато частіше доводиться мати справу з попередньо сформованим HTML-документом, куди потрібно як у шаблон підставити потрібні змінні. Для цього дуже добре підходить пакет Jinja.
Шаблони є важливим компонентом повнофункціональної веб-розробки. За допомогою Jinja можна створювати багаті, повноцінні шаблони, які забезпечують інтерфейс веб-застосунків на Python. Але в основному вони використовуються з веб-фреймворками.
Простий Web застосунок
Для стилізації застосунку ми будемо використовувати популярну бібліотеку Bootstrap https://getbootstrap.com/
використання спеціальної функції urllib.parse.urlparse повертає об'єкт ParseResult(scheme='', netloc='', path='/contact', params='', query='', fragment='')
Всі ці файли, які повинні повертатися з сервера, але не є файлами HTML, називають загальним словом статичні ресурси.
При надсиланні html файлу ми повідомили браузеру контент надсиланням наступного заголовка self.send_header('Content-type', 'text/html'). Це називається MIME types файлу. Так ми повідомляємо браузеру тип даних, які можуть бути передані за допомогою HTTP протоколу. Для визначення MIME types файлів у Python існує окремий модуль mimetypes.
Обробка форми виконується функцією do_POST.
Нехай ми ввели у формі такі дані, як на малюнку нижче, і натиснули кнопку Send. Браузер сформує дані та надішле їх нашому застосунку. Він збирає дані з тегів input, textarea тощо. Формує рядки зі значенням атрибуту name та атрибуту value, розділяючи їх знаком дорівнює. Наприклад, для <input class="form-control" name="username" type="text" /> у нашому випадку буде рядок виду username=krabaton. Остаточний рядок виходить з'єднанням через знак & всіх отриманих рядків з елементів введення.
Для отримання даних у застосунку з форми ми використовуємо функцію self.rfile.read, яка читає байт-рядок певного розміру. Розмір даних, що передаються, у байтах браузер (клієнт) передає через заголовок Content-Length. Тому наступним рядком коду ми отримуємо дані від браузера:
data = self.rfile.read(int(self.headers['Content-Length']))
print(data)
Це байт-рядок виду: b'username=krabaton&email=krabat%40test.com&message=Hello+my+friend'
Для форми з enctype="application/x-www-form-urlencoded" пробіли повинні бути замінені на "+", а також браузер застосовує до рядка метод encodeURIComponent . Щоб повернути дані до початкового вигляду, нам потрібно застосувати метод urllib.parse.unquote_plus
Ми розглянули роботу вебзастосунків за протоколом HTTP. Але Python також дозволяє нам виконувати передачу повідомлень між застосунками на нижчому рівні — за допомогою протоколу TCP/IP. Для цього він використовує сокети.
Сокет (Socket) — це програмний інтерфейс для забезпечення інформаційного обміну між процесами. Існують клієнтські та серверні сокети. Серверний сокет прослуховує певний порт, а клієнтський підключається до сервера. Щойно було встановлено з'єднання, починається обмін даними.
У Python для роботи з сокетами використовується модуль socket
Сокет — це системний ресурс і, як і файл, його потрібно повернути системі (закрити), що б не сталося. Для цього, як і у файлів, ми можемо скористатися менеджером контексту.
Крім цього, ще корисно вказати системі, що якщо застосунок не закрив з'єднання, то потрібно дозволити повторно відкрити тому самому порту. Для цього налаштуємо сокет:
sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
Операції із сокетами — за замовчуванням блокуючі. Це означає, що виклики connect, accept, recv, sendall не дадуть застосунку продовжувати роботу, доки не завершаться успішно. Але цю поведінку можна змінити
Для цього можна встановити прапорець: socket.setblocking(0)
Асинхронні сокети asyncio реалізовані саме за допомогою обробки результатів неблокуючих викликів при socket.setblocking(0)
Ви також можете скористатися фреймворком, який допомагає у створенні власних серверів socketserver.
https://docs.python.org/3.9/library/socketserver.html
Відповідно до способів взаємодії мережею, програми можна поділити на дві категорії:
без встановлення з'єднання (протокол UDP)
із встановленням з'єднання (протокол TCP)
Протоколи UDP та TCP працюють поверх мережевого IP-протоколу. Вони більше відомі під загальною назвою — стек протоколів TCP/IP
При передачі даних мережею завжди потрібно пам'ятати, що обмін відбувається сокетами (байтовими даними) і необхідно виконувати кодування та декодування даних.
Для зручності використання створені абстракції для роботи з протоколом HTTP. Вони побудовані поверх протоколу TCP/IP так, щоб не потрібно було використовувати сокети напряму. При надсиланні всіх видів HTTP-запитів у Python можна використовувати зовнішній модуль request або вбудований пакет urllib, та його метод urllib.request.
Модуль urllib містить все необхідне для роботи з HTTP. Ви можете вказати метод запиту, заголовок, додати дані в тіло запиту, отримати файл мережею, перевірити статус-код відповіді тощо.
Проте, все одно залишаються типові проблеми, з якими доводиться постійно стикатися:
зберігати кукі відкритих сесій;
перевикористовувати відкриті TCP з'єднання, щоб заощадити час та ресурси на створення нових для кожного запиту;
обробку таймаутів;
повторення запиту у разі обриву мережі.
Ці проблеми вирішені у сторонньому та дуже популярному пакеті requests.
Пакет "вміє" працювати з архівованими відповідями, JSON даними, різним кодуванням. Більшість "фіч" реалізовані всередині і просто працюють без вашої участі або налаштування. Наприклад, не потрібно вказувати кодування відповіді, воно автоматично буде розпізнано із заголовка відповіді. Не потрібно вказувати, що тіло відповіді стиснене, потрібний алгоритм стиснення буде застосований автоматично.


video 12.02.24
1:35:00 - ip utilities Linux
2:05:00 - web inspect in practice!!!
2:20:00 - Jinja2
video 14.02.24
0:32:00 - fncton decorator???
0:55:00 - dataclases
1:00:00 - dict([touple(a,b), touple(c,d)]) ->{a:b, c:d}
1:45:00 - TCP - перевіряє порядок надходження та комплектність пакетів - це важливо для HTTP, UDP - просто відправляє пакети, де комплектність не важлива - videostream (стрибкі кадрів), gaming (щоб один тормознутий не лагав всім)
2:10:00 - тест сервера з допомогою requests

M5
Найпростіший, найочевидніший та найінтуїтивніший спосіб написання застосунку — це виконання завдань в одному потоці по черзі — синхронна модель програмування (Synchronous model). Однак, істотну частину операцій читання/запису займає очікування відповіді.
Щоб якось використовувати час простою застосунку, ви можете виконувати завдання в кількох потоках — потокова модель програмування (Threaded model).
Але такий підхід ускладнює застосунок. Розробник не управляє тим, коли та якому потоку операційна система (ОС) віддасть контроль. Через це доводиться використовувати примітиви синхронізації (Lock, RLock, Semaphore, Barier та інші).
Накладні витрати призводять до того, що виконувати Python застосунок у більше ніж 10 потоках, зазвичай, не призводить до зростання продуктивності. А використання більше 100 потоків може бути повільнішим, ніж виконання тієї самої роботи синхронно в одному потоці.
Є архітектурний підхід, який поєднує в собі простоту та однозначність стану пам'яті однопотокового застосунку, та ефективне використання часу очікування (простою) під час IO операцій читання/запису.
Цей підхід — це цикл подій (Event loop) або асинхронна модель програмування (Asynchronous model).
Ключовим моментом, що дає можливість збільшити продуктивність асинхронних застосунків — це передача контролю під час IO операцій. 
Виходить, що не реалізувавши неблокуючі аналоги всіх IO операцій, не можна досягти приросту продуктивності асинхронного коду. Через це асинхронний підхід так повільно впроваджувався у Python. Адже операції, що не блокують IO, реалізувати набагато складніше своїх синхронних аналогів.
У Python, починаючи з версії 3.5, додали нові ключові слова: async та await. Був реалізований на рівні синтаксису цикл подій та код, який поміщений у модуль asyncio.
Щоб функція виконувалася в асинхронному режимі та виступала в ролі coroutine, перед її визначенням необхідно додати ключове слово async.
Оскільки далеко не всі IO операції у всіх пакетах реалізують async/await синтаксис, то часто трапляється, що потрібно виконати в асинхронному коді блокуючий IO виклик з пакета, де немає async/await аналога.Для цього можна скористатися вже відомим вам пакетом concurrent.futures та методом run_in_executor об'єкта loop (це і є Event loop).
Також можна перетворити CPU завдання, де відбуваються "важкі" обчислення, в асинхронний виклик, використовуючи ProcessPoolExecutor, замість ThreadPoolExecutor, тоді виклики будуть виконуватися в окремих процесах і не блокуватимуть основний Event loop.
Для асинхронної роботи з файлами існує низка пакетів. І почнемо ми з aiofile. Він виконує асинхронні операції за підтримки пакета asyncio.
Метод readline неоптимальний для невеликих рядків, оскільки не використовує повторно буфер читання. Якщо ви хочете читати файл порядково, будь ласка, уникайте використання async_open, замість цього використовуйте LineReader.
LineReader — помічник, який дуже ефективний, коли ви хочете прочитати файл лінійно та рядково. Він містить буфер і зчитуватиме фрагменти файлу частинами в буфер, де намагатиметься знайти рядки. Розмір фрагмента за замовчуванням складає 4 КБ.
Якщо ви пишете асинхронний код Python і хочете скористатися перевагами pathlib, але не хочете змішувати блокуюче та неблокуюче введення-виведення, ви можете звернутися до aiopath . API aiopath прямо збігається з API pathlib, але всі необхідні методи асинхронні.
Бібліотека aioshutil надає асинхронну версію функції модуля Shutil.
Модуль Shutil є синхронним, та його використання в асинхронних застосунках заблокує цикл подій і уповільнить роботу застосунку, aioshutil надає асинхронні дружні версії функцій модуля Shutil.
AIOHTTP — це асинхронний фреймворк, в якому реалізовано web-стек на основі asyncio в Python. В AIOHTTP реалізовані клієнтська частина та серверна частина. Реалізація HTTP і сам веб-сервер не використовують додаткові зовнішні бібліотеки. Вбудований веб-сервер реалізований на рівні, достатньому для використання в реальних навантажених проектах.
Відмінною рисою AIOHTTP є обов'язкове використання механізму сесій з'єднань.
Наступний фрагмент коду ми використовуємо, щоб уникнути помилки RuntimeError: Event loop is closed в системі Windows
if platform.system() == 'Windows':     	asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

Відповідь сервера завжди гарантовано містить заголовок і він буде доступним, щойно отримає відповідь. Тіло запиту може бути досить великим (файл великого розміру, потік даних) та AIOHTTP дає можливість обробити тільки заголовок, щоб вирішити завантажувати далі тіло запиту чи ні.
Типовий підхід використовувати одну сесію для з'єднання з одним сервісом — це значно прискорює виконання кількох запитів на один і той самий сервіс. У у такому разі ви можете передавати створену сесію як аргумент у функцію.
Поки що ми виконували тільки GET запити, але документація наводить приклади відправлення для інших методів HTTP:
session.post('http://httpbin.org/post', data=b'data')
session.put('http://httpbin.org/put', data=b'data')
session.delete('http://httpbin.org/delete')
session.head('http://httpbin.org/get')
session.options('http://httpbin.org/get')
session.patch('http://httpbin.org/patch', data=b'data')
На жаль, у мережевих запитах трапляються помилки. Це може бути неіснуючий шлях http://www.python.org/asdf або так звані "биті" посилання http://test. Їх потрібно не забувати опрацьовувати.
Технологія, де найпотужніше використовується інструмент async/await, – це WebSocket. WebSocket - це протокол двоспрямованого обміну даними, що характеризує повністю дуплексний характер взаємодії.
Основні переваги WebSocket у порівнянні з моделлю довгого опитування HTTP: https://ably.com/topic/long-polling
https://ably.com/blog/websockets-vs-long-polling
Для роботи з веб-сокетами потрібен Python версії 3.6.1 і вище.
URL ресурсу веб-сокету використовує власну схему, що починається з ws (або wss для безпечного підключення). Далі йде ім'я хосту та номер порту ws://localhost:8765.

video 19.02.24
0:33 a = [1] -> b=a->[1] но b, = a ->1
1:30 як залипає async

M6

Але якщо винести завдання генерації самих SQL запитів в окремий пакет/модуль? Тоді ми можемо зосередитись на описі таблиць як Python класів і, якщо необхідно, щось змінити в структурі даних, що зберігаються, ми робитимемо це тільки в одному місці. У всіх інших місцях нехай механізм генерації SQL запитів зробить це за нас, спираючись на певний опис структури даних в одному місці.
Саме це завдання вирішують усі ORM (Object Relationship Mapper) пакети. Вони всі є генераторами SQL запитів, що спираються на визначену в одному місці структуру даних та приховують реальний SQL, дозволяючи розробнику зосередитись на використанні лише однієї мови у застосунку.
SQLAlchemy — це найпопулярніший ORM-пакет, що спрощує і стандартизує роботу з SQL-сумісними базами даних.

from sqlalchemy import create_engine
engine = create_engine('sqlite:///:memory:', echo=True)

У цьому прикладі ми використовуємо SQLite базу даних у пам'яті, на диск нічого не записується.
Для початку потрібно розуміти, що SQLAlchemy містить кілька шарів (рівнів) організації. Найнижчий рівень організації — це ядро, core, на якому ви можете писати дуже близький до SQL коду Python код. Робота на core рівні не надто зручна і потрібна, в основному, тільки для дуже специфічних завдань.

Щоб працювати з базою даних так, ніби немає жодного окремого сховища, просто об'єкти Python "на льоту" зберігаються і так само "на льоту" доступні для роботи, в SQLAlchemy є високорівневіший механізм ORM.

from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

engine = create_engine('sqlite:///sqlalchemy_example.db')
DBSession = sessionmaker(bind=engine)
session = DBSession()

У цьому прикладі ми створюємо клас DBSession, об'єкти якого є окремими сесіями доступу до бази даних. Кожна така сесія може зберігати набір транзакцій і виконувати їх тільки коли це дійсно потрібно. Таке "ледаче" виконання зменшує навантаження на базу та прискорює роботу застосунку.
Сесія в ORM — це об'єкт, за допомогою якого ви можете управляти, коли саме накопичені зміни будуть застосовані до бази. Для цього є метод commit. Є методи для додавання одного або кількох об'єктів до бази (add, add_all).
У якості об'єкта, що сполучає стан бази та опис бази, в Python коді виступає Base, саме цей клас відповідає за "магію" синхронізації таблиць бази даних та їх описи в Python класах.
Щоб отримати дані з бази, можна скористатися методом query
Ви можете додавати фільтруючі умови, використовуючи метод filter, та інші SQL конструкції, використовуючи методи об'єкту Query (саме його повертає метод query). Конструювання запитів дуже спрощується за допомогою ORM, водночас це досить велика тема і рекомендуємо ознайомитися з документацією за посиланням.

SQLAlchemy середовище ORM надає функції відношень один-до-багатьох, багато-до-багатьох між двома об'єктами.
Клас Article зберігає у полі user_id зовнішній ключ на таблицю класу User. Так само для query запитів ми створюємо два відношення: articles у класі User та author у класі Article. Параметр back_populates пов'язує ці відношення між собою. Тепер ми можемо будувати двоспрямовані запити до таблиць.

Відношення багато до багатьох. Наприклад, запис зазвичай має кілька тегів. Між тегами та записами існує відношення багатьох до багатьох. Відношення багато до багатьох не може бути визначено безпосередньо, і його необхідно розбити на два відношення один до багатьох. Для цього використовують допоміжну таблицю для зв'язку між цими таблицями. Ми зараз не повністю розписуватимемо цей процес, тому що розберемо його в наступному розділі при розборі міграцій.
Якщо з якоїсь причини нам потрібно визначити відношення один до одного, то воно будується на відношенні один до багатьох за допомогою параметра uselist=False
Параметр backref є аналогом раніше розглянутого параметра back_populates. Відмінність у тому, що відношення з параметром backref достатньо оголосити в одному класі, щоб можна було б будувати двоспрямовані запити з query.

Міграція баз даних за допомогою Alembic
В ідеальному світі один раз описана база даних ніколи не змінює свою структуру. Наш світ не є ідеальним і бази даних постійно змінюються. Це досить болісний процес, особливо, якщо врахувати, що в базі часто вже є дані і потрібно їх правильно і акуратно перенести на нову схему.
Щоб якось зберегти історію змін бази даних, вигадали механізм міграцій. 
Щоб якось зменшити ймовірність помилок при зміні структури бази даних цей механізм спробували автоматизувати. SQLAlchemy не має свого інструмента міграцій. Для цього використовується окремий пакет — Alembic.
Насправді, Alembic — дуже потужний інструмент і він не зав'язаний жорстко на SQLAlchemy, може бути інструментом міграцій для широкого набору різних пакетів. Найчастіше, звичайно, застосовується в парі з SQLAlchemy.
Міграції, за своєю суттю, — це просто набір скриптів, що описують перехід від схеми A у схему B, і назад. Головна заслуга Alembic — це автогенерація таких скриптів. Коли ви змінюєте модель, що описує таблицю в базі, та з ваших змін Alembic може згенерувати дві функції: як перейти з поточного стану бази в описаний вами Python код і назад. Другий дуже важливий механізм Alembic — це історія всіх змін. Alembic зберігає весь ланцюжок всіх змін і може автоматизувати перехід до будь-якої точки у цій історії. По суті, аналог GIT лише для баз даних.

Асинхронні конектори та ORM
Більшість ORM з'явилася порівняно давно, до появи підтримки asyncio у Python. Саме з цієї причини майже всі ORM синхронні та конектори до баз також синхронні. Починаючи з Python 3.6 стало очевидним, що Python буде розвиватися у бік розширення підтримки asyncio, і будуть потрібні асинхронні ORM. SQLAlchemy з версії 1.4 та 2.0 підтримує asyncio синтаксис та дозволяє асинхронно працювати із базою даних. У будь-якому випадку для цього SQLAlchemy потрібні конектори до бази даних, що підтримують asyncio. Основні відмінності від "звичайного", синхронного підходу, описані на спеціальній сторінці.
Ви можете використовувати асинхронний режим роботи з SQLAlchemy, якщо ви використовуєте діалекти, сумісні з asyncio, такі, як asyncpg, aiomysql або aiopg. 
Для цього вам потрібно створити асинхронний движок за допомогою функції create_async_engine. Функція приймає URL з'єднання з додатковим параметром +asyncpg, +aiomysql або +aiopg. 
Наприклад: engine = create_async_engine("postgresql+asyncpg://user:password@host/database")
Потім ви можете використовувати асинхронний движок для отримання асинхронних з'єднань або транзакцій за допомогою методів AsyncEngine.connect() і AsyncEngine.begin(), які повертають асинхронні контекстні менеджери. Ви можете виконувати SQL-запити за допомогою методів AsyncConnection.execute() або AsyncConnection.stream(), які повертають об'єкти Result або AsyncResult відповідно.
Асинхронний режим роботи з SQLAlchemy дозволяє вам використовувати переваги asyncio для підвищення продуктивності та масштабованості вашого додатка. Однак він також вимагає від вас дотримання деяких правил і обмежень, таких як:
- Використання тільки одного event loop для всіх асинхронних операцій.
- Уникнення виклику синхронних методів і функцій всередині асинхронного коду.
- Використання методу run_sync для виконання синхронних операцій, таких як створення і видалення таблиць.
- Використання методу scalar або scalars для вилучення об'єктів з результатів ORM-запитів.
Є й повністю асинхронні ORM, які спочатку розроблялися з підтримкою asyncio:
    Gino
    Pony.
    Tortoise
    Peewee

