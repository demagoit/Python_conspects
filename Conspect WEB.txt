M1 22.01.24

SOLID - це важливо, 100% запитають на інтервью
автотести на pet-проектах - це важливий + у карму


M1 29.01.24
video 0:30 [scripts] - точка входу, запуск скрипта pipenv run my_app

M2
Python має пакет, який фактично став золотим стандартом — це black.
black суворо дотримується PEP8, але вносить низку своїх власних правил, які покращують сприйняття коду. За потреби, можна налаштувати black "під себе", але в 99% випадків стандартних налаштувань цілком достатньо.
PyCharm IDE: https://plugins.jetbrains.com/plugin/14321-blackconnect
VSCode: https://marketplace.visualstudio.com/items?itemName=ms-python.black-formatter

Статичні аналізатори, що найчастіше використовуються:
Pylint Flake8 MyPy Radon

Оскільки такі об'єкти, як списки, словники та кортежі містять інші об'єкти, іноді нам потрібно ввести підказки, щоб зазначити, які типи об'єктів вони містять. Для цього нам потрібно звернутись до модуля typing, який надає інструменти для опису типів.
from typing import List
Data = List[float | int]

Надаю вам те, що вам дуже допоможе:wink:
Шпаргалки по Docker
Робота з реєстрами та репозиторіями Docker:
- Вхід у реєстр: `docker login` або `docker login localhost:8080`
- Вихід з реєстру: `docker logout` або `docker logout localhost:8080`
- Пошук образу: `docker search nginx`
- Pull образа: `docker pull nginx`
- Push образа: `docker push eon01/nginx`
Початкові кроки з контейнерами:
- Створення контейнера: `docker create -t -i eon01/infinite --name infinite`
- Перший запуск контейнера: `docker run -it --name infinite -d eon01/infinite`
- Перейменування контейнера: `docker rename infinite infinity`
- Видалення контейнера: `docker rm infinite`
- Оновлення контейнера: `docker update --cpu-shares 512 -m 300M infinite`
Запуск та зупинка контейнерів:
- Запуск зупиненого контейнера: `docker start nginx`
- Остановка: `docker stop nginx`
- Перезавантаження: `docker restart nginx`
- Пауза: `docker pause nginx`
- Скасування паузи: `docker unpause nginx`
- Блокування: `docker wait nginx`
- Відправка сигналу SIGKILL: `docker kill nginx`
- Підключення до контейнера: `docker attach nginx`
Отримання інформації про контейнери:
- Перегляд запущених контейнерів: `docker ps`
- Логи контейнера: `docker logs infinite`
- Інформація про контейнер: `docker inspect infinite`
- Події контейнера: `docker events infinite`
- Публічні порти: `docker port infinite`
- Процеси у контейнері: `docker top infinite`
- Ресурси контейнера: `docker stats infinite`
- Зміни в ФС контейнера: `docker diff infinite`
Управління образами:
- Список образів: `docker images`
- Створення образа: `docker build .`
- Видалення образа: `docker rmi nginx`
- Завантаження образа: `docker load < ubuntu.tar.gz`
- Збереження образа: `docker save busybox > ubuntu.tar`
- Історія образа: `docker history`
- Створення образа з контейнера: `docker commit nginx`
- Тегування образа: `docker tag nginx eon01/nginx`
- Push образа: `docker push eon01/nginx`
Мережа Docker:
- Створення мережі: `docker network create -d overlay MyOverlayNetwork`
- Видалення мережі: `docker network rm MyOverlayNetwork`
- Перегляд мереж: `docker network ls`
- Інформація про мережу: `docker network inspect MyOverlayNetwork`
- Підключення контейнера до мережі: `docker network connect MyOverlayNetwork nginx`
- Відключення контейнера від мережі: `docker network disconnect MyOverlayNetwork nginx`
Очищення Docker:
- Видалення контейнера: `docker rm nginx`
- Видалення контейнера та його тома: `docker rm -v nginx`
Docker зберігає кеш для прискорення процесу збірки образів. Видаліть його так:docker system prune -a
Шпаргалки для роботи з GitHub
“echo “# назва” >> README.md” - створює файл README.md
“git init” - ініціалізація репозиторію
“git add README.md” - додавання файлу README.md до проекту
“git commit -m “first commit”" - фіксація змін у репозиторії з певним повідомленням
“git remote add origin https://github.com/stanruss/название.git” - встановлення з‘єднання з віддаленим репозиторієм
“git push -u origin master” - відправлення змін на віддалений сервер
“git log --oneline” - перегляд усіх комітів
“git checkout .” - відновлення всього
“git checkout “код коммита“” - повернення до стану певного коміту
“git checkout master” - повернення до гілки master
Для відновлення файлів на локальному комп’ютері:
```git fetch --all
git reset --hard origin/master``` - скидання змін до останньої версії на сервері
“git add text.txt” - додавання файлу до репозиторію
“git rm text.txt” - видалення файлу
“git status” - поточний стан репозиторію
“git commit -a -m “Commit description”" - створення коміту з усіма змінами
“git push origin” - синхронізація всіх гілок локального репозиторію з віддаленим
“git push origin master” - синхронізація гілки master з віддаленим репозиторієм
“git push origin HEAD” - відправлення поточної гілки на сервер
“git pull origin” - синхронізація всіх гілок з сервером
“git pull origin master” - синхронізація гілки master з сервером
“git fetch origin” - завантаження всіх гілок з серверу без злиття з локальними
“git merge some_branch” - злиття гілки some_branch з поточною
“git branch -d some_branch” - видалення гілки (після злиття)
“git branch -D some_branch” - примусове видалення гілки
“git show <commit-hash>” - перегляд змін в певному коміті
“git push origin :branch-name” - видалення гілки з серверу
“git reset --hard <commit-hash>” - повернення до певного коміту і видалення наступних змін
“git push -f” - примусове відправлення змін на сервер
“git clean -f” - видалення неіндексованих файлів (edited) 
Docker зберігає кеш для прискорення процесу збірки образів, але це займає місце. Видаліть його так: docker system prune -a

M3 
Процес — область пам'яті (віртуальна) + набір ресурсів + 1 і більше потоків.
Потік — послідовність інструкцій та системних викликів всередині процесу.

Завдання, які виконують операції введення/виведення (читання/запис файлів, запити в мережі тощо), називаються IO (Input Output)-bound завданнями. Домогтися паралелізму виконання IO завдань у Python можна, використовуючи потоки.
Інший тип блокуючих викликів — це важкі з точки зору обчислень операції. 
Такі завдання називаються CPU-bound завданнями. Як і для IO-bound завдань, можна винести виконання блокуючих операцій (складних обчислень) в окремий потік, щоб застосунок продовжував взаємодіяти з користувачем, здійснюючи обчислення.

Однак потрібно пам'ятати, що асинхронний код завжди на порядок складніший для розуміння та відлагодження.
Загальне правило для програмування будь-якою мовою: якщо є можливість обійтися синхронним кодом, то так і потрібно зробити.
https://medium.com/swift-india/concurrency-parallelism-threads-processes-async-and-sync-related-39fd951bc61d

Потоки можуть виконуватися дійсно паралельно (якщо ядер процесора більше 1), процеси — тим більше. Але у Python є механізм, який примусово блокує виконання коду різними потоками одного Python процесу в один і той самий час.
Як обійти GIL:
1. Написати частину коду, яку потрібно запускати паралельно, на Cython і використовувати потоки.
2. Використовувати Multiprocessing.

https://docs.python.org/3/library/concurrency.html

Щоб створити потік, найпростіше імпортувати клас Thread з модуля threading і наслідуватись від цього класу. Далі вам потрібно визначити метод run у вашого класу, цей метод буде виконуватись в окремому потоці. Щоб розпочати виконання коду в окремому потоці, потрібно викликати метод start, який визначений у Thread.
Є інший спосіб виконати код окремого потоку. Для цього потрібно, щоб код виконання був функтором (функцією або класом, який має метод __call__). Тоді об'єкт можна передати як іменований аргумент target у Thread.
Або у процесі створення екземпляра класу Thread можна передати аргументу target функцію та передати їй аргументи як кортеж args у Thread.
Основний потік спочатку запискає дочірні потокі, робить свій вивід і закінчується, а після нього дочірні потоки MyThread виводять своє повідомленя, і тільки після цього скрипт завершився.
Коли потрібно в основному застосунку дочекатися виконання потоку, можна скористатися блокуючим методом join.
Ви також можете перевірити — чи виконується потік, викликавши метод is_alive.
Екземпляри класу Timer починають працювати з деякою затримкою, яку визначає програміст. Крім того, ці потоки можна скасувати будь-якої миті методом .cancel() в період затримки.
Оскільки ОС може на будь-якому виклику перервати виконання потоку та передати контроль іншому потоку, ви не можете бути впевненим, що робота із загальним ресурсом буде коректно завершеною і ресурс не опиниться в невизначеному стані.
Для цього є механізм блокування. У Python є два примітива блокувань: Lock та RLock. Lock трохи швидший і більш низькорівневий, але він не рекурсивний і може бути ситуація потрапляння в DeadLock, коли виконання коду заблокується, кілька потоків чекатимуть, доки хтось віддасть Lock, а його ніхто ніколи вже не віддасть. RLock трохи повільніший, зате виключає взаємне блокування (дозволяя одному потоку мати доступ до ресурсів, які він сам заблокував і ще не розблокував), але розблокувати його треба стільки ж раз, скільки було заблоковано.
Блокування ресурсу досягається виконанням команди lock.acquire(), відпускає lock.release()
Але найчастіше для блокування використовують контекст виконання with lock.
Другий примітив синхронізації — це семафори.
Семафори підходять до блокування іншим шляхом та вказують, що кілька потоків можуть користуватися ресурсом одночасно і цим обмежують кількість потоків. Наприклад, ми не хочемо надсилати десятки тисяч запитів до мережі одночасно, щоб не створювати навантаження на обладнання і вкажемо семафор, щоб не більше ста потоків могли одночасно надсилати запити.
Наступним кроком ми розглянемо синхронізацію роботи потоків за допомогою умов та подій.
Є примітиви синхронізації, які дозволяють потокам очікувати сигнал від інших потоків — це Condition.
Методи керування: condition.wait(), condition.notify_all(), condition.notify()
Інший примітив синхронізації — це потокобезпечний прапорець класу Event.
event.set(), event.wait(), event.clear(), event.is_set()
Виникає закономірне питання, навіщо, якщо результат той самий? Справа в тому, що ми можемо керувати виконанням, перезапуском та зупинкою роботи потоків через клас Event. Наприклад, ми перериваємо виконання потоку, який працює в нескінченному циклі та інакше просто ніколи не завершиться, перевіркою break if event_for_exit.is_set().
Останній примітив синхронізації, який ми розглянемо в Python — це бар'єр Barrier.Він дозволяє задати умову, щоб кілька потоків продовжили роботу лише після того, як задане число потоків добереться у виконанні коду до цього "бар'єру". 
Потік може дістатися бар'єру і чекати його за допомогою функції wait(). Це блокуючий виклик, який повернеться, коли решта потоків (попередньо налаштована кількість barrier = Barrier(5)) дістануться бар'єру.
Функція очікування wait() повертає ціле число, яке вказує на кількість учасників, яких ще треба дочекатися. Якщо потік був останнім, що прибув, то повернене значення буде нульовим.
Головна вимога, щоб кількість потоків, що запускаються була кратною кількості бар'єру.
В Python існує ще один механізм написання асинхронного коду. Ви можете скористатися пакетом concurrent.futures. Він дозволяє піднятися на вищий рівень абстракції, коли вам просто потрібно паралельно виконати ряд однотипних завдань і немає необхідності вдаватися до низькорівневих деталей реалізації.
Основна ідея полягає у використанні реалізації абстрактного класу Executor. У concurrent.futures є дві реалізації цього абстрактного базового класу: ProcessPoolExecutor — для виконання коду окремих процесів (з ним ми познайомимося пізніше) та ThreadPoolExecutor — для виконання в окремих потоках.
Кожен такий Executor приховує набір потоків або процесів, яким ви можете дати роботу та отримати результат її виконання. Вам не потрібно вручну управляти створенням потоків та їх коректним завершенням.
Звичайно, все ще потрібно пам'ятати про доступ до загальних ресурсів та примітиви синхронізації.

Пакет multiprocessing — це пакет для виконання коду в окремих процесах з інтерфейсом подібним до інтерфейсу пакета threading.
Основна причина появи multiprocessing — це GIL (Global Interpreter Lock) і той факт, що threading API не дозволяє розпаралелювати CPU-bound завдання. Оскільки в один момент часу завжди виконується код тільки в одному потоці, навіть на багатоядерних сучасних процесорах, отримати приріст продуктивності для завдань, пов'язаних з інтенсивними обчисленнями, за допомогою threading не вийде.
Для використання процесів необхідно імпортувати клас Process модуля multiprocessing. З ним можна працювати декількома способами:
1.У процесі створення екземпляра класу Process іменованому аргументу target передати функцію, яка буде виконуватися в окремому процесі
2.Реалізувати похідний клас від класу Process та перевизначити метод run
https://docs.python.org/3.8/library/multiprocessing.html#programming-guidelines
Залежно від платформи multiprocessing підтримує 3 способи створення нового процесу:
1.spawn — запускає новий процес Python, наслідуються лише ресурси, необхідні для запуску run(). Присутній в Unix і Windows. Є способом за замовчуванням для Windows і macOS.
2.fork — дочірній процес, що є точною копією батьківського (включаючи всі потоки), доступний тільки на Unix. За замовчуванням використовується на Unix. Зробити безпечний fork досить проблематично і це може бути причиною неочевидних проблем.
3.forkserver — створюється процес-фабрика (сервер для породження процесів за запитом). Наслідуються тільки необхідні ресурси, що використовуються fork для запуску нового процесу, але завдяки однопотоковій реалізації процесу-фабрики, це робиться безпечно. Доступний тільки на Unix платформах з підтримкою передачі файлових дескрипторів через pipes (що може суперечити безпековій політиці на багатьох системах).
Для вибору методу використовується multiprocessing.set_start_method(method)
Для міжпроцесорної взаємодії виокремлюють наступні інструменти:
файли;
сокети;
канали (всі POSIX ОС);
роздільна пам'ять (всі POSIX ОС);
семафори (всі POSIX ОС);
сигнали або переривання (крім Windows);
семафори (всі POSIX ОС);
черга повідомлень;
У будь-якому разі, крім загальної пам'яті, для обміну даними між процесами всі об'єкти серіалізуються та десеріалізуються. Цей додатковий крок створює навантаження на CPU.


Пул процесів з multiprocessing дає більше контролю, ніж пул з concurrent.futures.
Основні можливості:
розбиває вхідну послідовність на блоки та виконує паралельну обробку поблоково, так можна зменшити обсяг використовуваної пам'яті;
асинхронне виконання трохи прискорює отримання результатів, якщо порядок не важливий;
передача кортежу аргументів у target-функцію;

Пакет concurrent.futures також реалізує API Executor для пулу процесів у класі ProcessPoolExecutor.
Основні можливості обмежені API Executor. Зручно використовувати ProcessPoolExecutor там, де потрібно виконати CPU-bound завдання в async коді та реалізовано саме для підтримки виконання блокуючих CPU-bound завдань в async застосунках.

video 05.02.24

1:15 - daemon=TRUE вбиває всі потоки, якщо закінчився основний

video 07.02.24
IO-bound задачи нормально оптимізує навіть Python-scheduler (multithread на одному GIL)
CPU-bound задачи Python-scheduler оптимізує погано (треба multiprocess на різних GIL)
в multiprocess є різниця при використанні root-loger та custom-logger

0:38 для widows доступен лише spawn, fork та forkserver недоступні

2:00 у pipe child процеси є копіями головного процесу, тому мають доступ до всіх globals, в т.ч. до всіх оголошених пайпів

2:25 queue.task_done() треба робити після завершення кожної таски в процессі, а не по завершенню процессу, інакше зависне в головному процесі на queue.join() бо  ніколи не отримає підтвердження виконання всіх тасков.

M4
У моделі OSI мережеві функції розподілені між сімома рівнями. Кожному рівню відповідають різні мережеві операції, обладнання та протоколи.
https://textbook.edu.goit.global/python-web-textbook/uk/docs/additional/web/osi
HTTP є синхронним протоколом. Це означає, що клієнт надіслав запит серверу і поки чекає від нього відповідь, наступні запити надіслати не може. Також варто відзначити, що HTTP — протокол без стану. Тобто сервер не зберігає інформацію про користувача між запитами.
Для збереження стану користувача в системі використовується механізм cookie або сесій.
Існує наступна традиційна форма запису `URL:
<scheme>://[<login>[:<password>]@]<host>[:<port>]][/<path>][?<query>][#<fragment>]
Коди стану відповідей сервера
https://en.wikipedia.org/wiki/List_of_HTTP_status_codes
https://docs.python.org/3.10/library/http.html?highlight=http
У Python для роботи з протоколом HTTP можна скористатися пакетом http, який реалізує дуже простий HTTP-сервер.Його не варто використовувати для "бойових" застосунків, але для навчання та розуміння основних механік роботи вебзастосунку його цілком достатньо.
https://docs.python.org/3.9/library/http.server.html
для верстки простіше використовувати готові інструменти на кшталт Bootstrap — вільний набір інструментів для створення сайтів та вебзастосунків. 
https://getbootstrap.com/
Створення чистих HTML-документів не є частим завданням у Python. Набагато частіше доводиться мати справу з попередньо сформованим HTML-документом, куди потрібно як у шаблон підставити потрібні змінні. Для цього дуже добре підходить пакет Jinja.
Шаблони є важливим компонентом повнофункціональної веб-розробки. За допомогою Jinja можна створювати багаті, повноцінні шаблони, які забезпечують інтерфейс веб-застосунків на Python. Але в основному вони використовуються з веб-фреймворками.
Простий Web застосунок
Для стилізації застосунку ми будемо використовувати популярну бібліотеку Bootstrap https://getbootstrap.com/
використання спеціальної функції urllib.parse.urlparse повертає об'єкт ParseResult(scheme='', netloc='', path='/contact', params='', query='', fragment='')
Всі ці файли, які повинні повертатися з сервера, але не є файлами HTML, називають загальним словом статичні ресурси.
При надсиланні html файлу ми повідомили браузеру контент надсиланням наступного заголовка self.send_header('Content-type', 'text/html'). Це називається MIME types файлу. Так ми повідомляємо браузеру тип даних, які можуть бути передані за допомогою HTTP протоколу. Для визначення MIME types файлів у Python існує окремий модуль mimetypes.
Обробка форми виконується функцією do_POST.
Нехай ми ввели у формі такі дані, як на малюнку нижче, і натиснули кнопку Send. Браузер сформує дані та надішле їх нашому застосунку. Він збирає дані з тегів input, textarea тощо. Формує рядки зі значенням атрибуту name та атрибуту value, розділяючи їх знаком дорівнює. Наприклад, для <input class="form-control" name="username" type="text" /> у нашому випадку буде рядок виду username=krabaton. Остаточний рядок виходить з'єднанням через знак & всіх отриманих рядків з елементів введення.
Для отримання даних у застосунку з форми ми використовуємо функцію self.rfile.read, яка читає байт-рядок певного розміру. Розмір даних, що передаються, у байтах браузер (клієнт) передає через заголовок Content-Length. Тому наступним рядком коду ми отримуємо дані від браузера:
data = self.rfile.read(int(self.headers['Content-Length']))
print(data)
Це байт-рядок виду: b'username=krabaton&email=krabat%40test.com&message=Hello+my+friend'
Для форми з enctype="application/x-www-form-urlencoded" пробіли повинні бути замінені на "+", а також браузер застосовує до рядка метод encodeURIComponent . Щоб повернути дані до початкового вигляду, нам потрібно застосувати метод urllib.parse.unquote_plus
Ми розглянули роботу вебзастосунків за протоколом HTTP. Але Python також дозволяє нам виконувати передачу повідомлень між застосунками на нижчому рівні — за допомогою протоколу TCP/IP. Для цього він використовує сокети.
Сокет (Socket) — це програмний інтерфейс для забезпечення інформаційного обміну між процесами. Існують клієнтські та серверні сокети. Серверний сокет прослуховує певний порт, а клієнтський підключається до сервера. Щойно було встановлено з'єднання, починається обмін даними.
У Python для роботи з сокетами використовується модуль socket
Сокет — це системний ресурс і, як і файл, його потрібно повернути системі (закрити), що б не сталося. Для цього, як і у файлів, ми можемо скористатися менеджером контексту.
Крім цього, ще корисно вказати системі, що якщо застосунок не закрив з'єднання, то потрібно дозволити повторно відкрити тому самому порту. Для цього налаштуємо сокет:
sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
Операції із сокетами — за замовчуванням блокуючі. Це означає, що виклики connect, accept, recv, sendall не дадуть застосунку продовжувати роботу, доки не завершаться успішно. Але цю поведінку можна змінити
Для цього можна встановити прапорець: socket.setblocking(0)
Асинхронні сокети asyncio реалізовані саме за допомогою обробки результатів неблокуючих викликів при socket.setblocking(0)
Ви також можете скористатися фреймворком, який допомагає у створенні власних серверів socketserver.
https://docs.python.org/3.9/library/socketserver.html
Відповідно до способів взаємодії мережею, програми можна поділити на дві категорії:
без встановлення з'єднання (протокол UDP)
із встановленням з'єднання (протокол TCP)
Протоколи UDP та TCP працюють поверх мережевого IP-протоколу. Вони більше відомі під загальною назвою — стек протоколів TCP/IP
При передачі даних мережею завжди потрібно пам'ятати, що обмін відбувається сокетами (байтовими даними) і необхідно виконувати кодування та декодування даних.
Для зручності використання створені абстракції для роботи з протоколом HTTP. Вони побудовані поверх протоколу TCP/IP так, щоб не потрібно було використовувати сокети напряму. При надсиланні всіх видів HTTP-запитів у Python можна використовувати зовнішній модуль request або вбудований пакет urllib, та його метод urllib.request.
Модуль urllib містить все необхідне для роботи з HTTP. Ви можете вказати метод запиту, заголовок, додати дані в тіло запиту, отримати файл мережею, перевірити статус-код відповіді тощо.
Проте, все одно залишаються типові проблеми, з якими доводиться постійно стикатися:
зберігати кукі відкритих сесій;
перевикористовувати відкриті TCP з'єднання, щоб заощадити час та ресурси на створення нових для кожного запиту;
обробку таймаутів;
повторення запиту у разі обриву мережі.
Ці проблеми вирішені у сторонньому та дуже популярному пакеті requests.
Пакет "вміє" працювати з архівованими відповідями, JSON даними, різним кодуванням. Більшість "фіч" реалізовані всередині і просто працюють без вашої участі або налаштування. Наприклад, не потрібно вказувати кодування відповіді, воно автоматично буде розпізнано із заголовка відповіді. Не потрібно вказувати, що тіло відповіді стиснене, потрібний алгоритм стиснення буде застосований автоматично.


video 12.02.24
1:35:00 - ip utilities Linux
2:05:00 - web inspect in practice!!!
2:20:00 - Jinja2
video 14.02.24
0:32:00 - fncton decorator???
0:55:00 - dataclases
1:00:00 - dict([touple(a,b), touple(c,d)]) ->{a:b, c:d}
1:45:00 - TCP - перевіряє порядок надходження та комплектність пакетів - це важливо для HTTP, UDP - просто відправляє пакети, де комплектність не важлива - videostream (стрибкі кадрів), gaming (щоб один тормознутий не лагав всім)
2:10:00 - тест сервера з допомогою requests

M5
Найпростіший, найочевидніший та найінтуїтивніший спосіб написання застосунку — це виконання завдань в одному потоці по черзі — синхронна модель програмування (Synchronous model). Однак, істотну частину операцій читання/запису займає очікування відповіді.
Щоб якось використовувати час простою застосунку, ви можете виконувати завдання в кількох потоках — потокова модель програмування (Threaded model).
Але такий підхід ускладнює застосунок. Розробник не управляє тим, коли та якому потоку операційна система (ОС) віддасть контроль. Через це доводиться використовувати примітиви синхронізації (Lock, RLock, Semaphore, Barier та інші).
Накладні витрати призводять до того, що виконувати Python застосунок у більше ніж 10 потоках, зазвичай, не призводить до зростання продуктивності. А використання більше 100 потоків може бути повільнішим, ніж виконання тієї самої роботи синхронно в одному потоці.
Є архітектурний підхід, який поєднує в собі простоту та однозначність стану пам'яті однопотокового застосунку, та ефективне використання часу очікування (простою) під час IO операцій читання/запису.
Цей підхід — це цикл подій (Event loop) або асинхронна модель програмування (Asynchronous model).
Ключовим моментом, що дає можливість збільшити продуктивність асинхронних застосунків — це передача контролю під час IO операцій. 
Виходить, що не реалізувавши неблокуючі аналоги всіх IO операцій, не можна досягти приросту продуктивності асинхронного коду. Через це асинхронний підхід так повільно впроваджувався у Python. Адже операції, що не блокують IO, реалізувати набагато складніше своїх синхронних аналогів.
У Python, починаючи з версії 3.5, додали нові ключові слова: async та await. Був реалізований на рівні синтаксису цикл подій та код, який поміщений у модуль asyncio.
Щоб функція виконувалася в асинхронному режимі та виступала в ролі coroutine, перед її визначенням необхідно додати ключове слово async.
Оскільки далеко не всі IO операції у всіх пакетах реалізують async/await синтаксис, то часто трапляється, що потрібно виконати в асинхронному коді блокуючий IO виклик з пакета, де немає async/await аналога.Для цього можна скористатися вже відомим вам пакетом concurrent.futures та методом run_in_executor об'єкта loop (це і є Event loop).
Також можна перетворити CPU завдання, де відбуваються "важкі" обчислення, в асинхронний виклик, використовуючи ProcessPoolExecutor, замість ThreadPoolExecutor, тоді виклики будуть виконуватися в окремих процесах і не блокуватимуть основний Event loop.
Для асинхронної роботи з файлами існує низка пакетів. І почнемо ми з aiofile. Він виконує асинхронні операції за підтримки пакета asyncio.
Метод readline неоптимальний для невеликих рядків, оскільки не використовує повторно буфер читання. Якщо ви хочете читати файл порядково, будь ласка, уникайте використання async_open, замість цього використовуйте LineReader.
LineReader — помічник, який дуже ефективний, коли ви хочете прочитати файл лінійно та рядково. Він містить буфер і зчитуватиме фрагменти файлу частинами в буфер, де намагатиметься знайти рядки. Розмір фрагмента за замовчуванням складає 4 КБ.
Якщо ви пишете асинхронний код Python і хочете скористатися перевагами pathlib, але не хочете змішувати блокуюче та неблокуюче введення-виведення, ви можете звернутися до aiopath . API aiopath прямо збігається з API pathlib, але всі необхідні методи асинхронні.
Бібліотека aioshutil надає асинхронну версію функції модуля Shutil.
Модуль Shutil є синхронним, та його використання в асинхронних застосунках заблокує цикл подій і уповільнить роботу застосунку, aioshutil надає асинхронні дружні версії функцій модуля Shutil.
AIOHTTP — це асинхронний фреймворк, в якому реалізовано web-стек на основі asyncio в Python. В AIOHTTP реалізовані клієнтська частина та серверна частина. Реалізація HTTP і сам веб-сервер не використовують додаткові зовнішні бібліотеки. Вбудований веб-сервер реалізований на рівні, достатньому для використання в реальних навантажених проектах.
Відмінною рисою AIOHTTP є обов'язкове використання механізму сесій з'єднань.
Наступний фрагмент коду ми використовуємо, щоб уникнути помилки RuntimeError: Event loop is closed в системі Windows
if platform.system() == 'Windows':     	asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

Відповідь сервера завжди гарантовано містить заголовок і він буде доступним, щойно отримає відповідь. Тіло запиту може бути досить великим (файл великого розміру, потік даних) та AIOHTTP дає можливість обробити тільки заголовок, щоб вирішити завантажувати далі тіло запиту чи ні.
Типовий підхід використовувати одну сесію для з'єднання з одним сервісом — це значно прискорює виконання кількох запитів на один і той самий сервіс. У у такому разі ви можете передавати створену сесію як аргумент у функцію.
Поки що ми виконували тільки GET запити, але документація наводить приклади відправлення для інших методів HTTP:
session.post('http://httpbin.org/post', data=b'data')
session.put('http://httpbin.org/put', data=b'data')
session.delete('http://httpbin.org/delete')
session.head('http://httpbin.org/get')
session.options('http://httpbin.org/get')
session.patch('http://httpbin.org/patch', data=b'data')
На жаль, у мережевих запитах трапляються помилки. Це може бути неіснуючий шлях http://www.python.org/asdf або так звані "биті" посилання http://test. Їх потрібно не забувати опрацьовувати.
Технологія, де найпотужніше використовується інструмент async/await, – це WebSocket. WebSocket - це протокол двоспрямованого обміну даними, що характеризує повністю дуплексний характер взаємодії.
Основні переваги WebSocket у порівнянні з моделлю довгого опитування HTTP: https://ably.com/topic/long-polling
https://ably.com/blog/websockets-vs-long-polling
Для роботи з веб-сокетами потрібен Python версії 3.6.1 і вище.
URL ресурсу веб-сокету використовує власну схему, що починається з ws (або wss для безпечного підключення). Далі йде ім'я хосту та номер порту ws://localhost:8765.

video 19.02.24
0:33 a = [1] -> b=a->[1] но b, = a ->1
1:30 як залипає async

M6

Але якщо винести завдання генерації самих SQL запитів в окремий пакет/модуль? Тоді ми можемо зосередитись на описі таблиць як Python класів і, якщо необхідно, щось змінити в структурі даних, що зберігаються, ми робитимемо це тільки в одному місці. У всіх інших місцях нехай механізм генерації SQL запитів зробить це за нас, спираючись на певний опис структури даних в одному місці.
Саме це завдання вирішують усі ORM (Object Relationship Mapper) пакети. Вони всі є генераторами SQL запитів, що спираються на визначену в одному місці структуру даних та приховують реальний SQL, дозволяючи розробнику зосередитись на використанні лише однієї мови у застосунку.
SQLAlchemy — це найпопулярніший ORM-пакет, що спрощує і стандартизує роботу з SQL-сумісними базами даних.

from sqlalchemy import create_engine
engine = create_engine('sqlite:///:memory:', echo=True)

У цьому прикладі ми використовуємо SQLite базу даних у пам'яті, на диск нічого не записується.
Для початку потрібно розуміти, що SQLAlchemy містить кілька шарів (рівнів) організації. Найнижчий рівень організації — це ядро, core, на якому ви можете писати дуже близький до SQL коду Python код. Робота на core рівні не надто зручна і потрібна, в основному, тільки для дуже специфічних завдань.

Щоб працювати з базою даних так, ніби немає жодного окремого сховища, просто об'єкти Python "на льоту" зберігаються і так само "на льоту" доступні для роботи, в SQLAlchemy є високорівневіший механізм ORM.

from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

engine = create_engine('sqlite:///sqlalchemy_example.db')
DBSession = sessionmaker(bind=engine)
session = DBSession()

У цьому прикладі ми створюємо клас DBSession, об'єкти якого є окремими сесіями доступу до бази даних. Кожна така сесія може зберігати набір транзакцій і виконувати їх тільки коли це дійсно потрібно. Таке "ледаче" виконання зменшує навантаження на базу та прискорює роботу застосунку.
Сесія в ORM — це об'єкт, за допомогою якого ви можете управляти, коли саме накопичені зміни будуть застосовані до бази. Для цього є метод commit. Є методи для додавання одного або кількох об'єктів до бази (add, add_all).
У якості об'єкта, що сполучає стан бази та опис бази, в Python коді виступає Base, саме цей клас відповідає за "магію" синхронізації таблиць бази даних та їх описи в Python класах.
Щоб отримати дані з бази, можна скористатися методом query
Ви можете додавати фільтруючі умови, використовуючи метод filter, та інші SQL конструкції, використовуючи методи об'єкту Query (саме його повертає метод query). Конструювання запитів дуже спрощується за допомогою ORM, водночас це досить велика тема і рекомендуємо ознайомитися з документацією за посиланням.

SQLAlchemy середовище ORM надає функції відношень один-до-багатьох, багато-до-багатьох між двома об'єктами.
Клас Article зберігає у полі user_id зовнішній ключ на таблицю класу User. Так само для query запитів ми створюємо два відношення: articles у класі User та author у класі Article. Параметр back_populates пов'язує ці відношення між собою. Тепер ми можемо будувати двоспрямовані запити до таблиць.

Відношення багато до багатьох. Наприклад, запис зазвичай має кілька тегів. Між тегами та записами існує відношення багатьох до багатьох. Відношення багато до багатьох не може бути визначено безпосередньо, і його необхідно розбити на два відношення один до багатьох. Для цього використовують допоміжну таблицю для зв'язку між цими таблицями. Ми зараз не повністю розписуватимемо цей процес, тому що розберемо його в наступному розділі при розборі міграцій.
Якщо з якоїсь причини нам потрібно визначити відношення один до одного, то воно будується на відношенні один до багатьох за допомогою параметра uselist=False
Параметр backref є аналогом раніше розглянутого параметра back_populates. Відмінність у тому, що відношення з параметром backref достатньо оголосити в одному класі, щоб можна було б будувати двоспрямовані запити з query.

Міграція баз даних за допомогою Alembic
В ідеальному світі один раз описана база даних ніколи не змінює свою структуру. Наш світ не є ідеальним і бази даних постійно змінюються. Це досить болісний процес, особливо, якщо врахувати, що в базі часто вже є дані і потрібно їх правильно і акуратно перенести на нову схему.
Щоб якось зберегти історію змін бази даних, вигадали механізм міграцій. 
Щоб якось зменшити ймовірність помилок при зміні структури бази даних цей механізм спробували автоматизувати. SQLAlchemy не має свого інструмента міграцій. Для цього використовується окремий пакет — Alembic.
Насправді, Alembic — дуже потужний інструмент і він не зав'язаний жорстко на SQLAlchemy, може бути інструментом міграцій для широкого набору різних пакетів. Найчастіше, звичайно, застосовується в парі з SQLAlchemy.
Міграції, за своєю суттю, — це просто набір скриптів, що описують перехід від схеми A у схему B, і назад. Головна заслуга Alembic — це автогенерація таких скриптів. Коли ви змінюєте модель, що описує таблицю в базі, та з ваших змін Alembic може згенерувати дві функції: як перейти з поточного стану бази в описаний вами Python код і назад. Другий дуже важливий механізм Alembic — це історія всіх змін. Alembic зберігає весь ланцюжок всіх змін і може автоматизувати перехід до будь-якої точки у цій історії. По суті, аналог GIT лише для баз даних.

Асинхронні конектори та ORM
Більшість ORM з'явилася порівняно давно, до появи підтримки asyncio у Python. Саме з цієї причини майже всі ORM синхронні та конектори до баз також синхронні. Починаючи з Python 3.6 стало очевидним, що Python буде розвиватися у бік розширення підтримки asyncio, і будуть потрібні асинхронні ORM. SQLAlchemy з версії 1.4 та 2.0 підтримує asyncio синтаксис та дозволяє асинхронно працювати із базою даних. У будь-якому випадку для цього SQLAlchemy потрібні конектори до бази даних, що підтримують asyncio. Основні відмінності від "звичайного", синхронного підходу, описані на спеціальній сторінці.
Ви можете використовувати асинхронний режим роботи з SQLAlchemy, якщо ви використовуєте діалекти, сумісні з asyncio, такі, як asyncpg, aiomysql або aiopg. 
Для цього вам потрібно створити асинхронний движок за допомогою функції create_async_engine. Функція приймає URL з'єднання з додатковим параметром +asyncpg, +aiomysql або +aiopg. 
Наприклад: engine = create_async_engine("postgresql+asyncpg://user:password@host/database")
Потім ви можете використовувати асинхронний движок для отримання асинхронних з'єднань або транзакцій за допомогою методів AsyncEngine.connect() і AsyncEngine.begin(), які повертають асинхронні контекстні менеджери. Ви можете виконувати SQL-запити за допомогою методів AsyncConnection.execute() або AsyncConnection.stream(), які повертають об'єкти Result або AsyncResult відповідно.
Асинхронний режим роботи з SQLAlchemy дозволяє вам використовувати переваги asyncio для підвищення продуктивності та масштабованості вашого додатка. Однак він також вимагає від вас дотримання деяких правил і обмежень, таких як:
- Використання тільки одного event loop для всіх асинхронних операцій.
- Уникнення виклику синхронних методів і функцій всередині асинхронного коду.
- Використання методу run_sync для виконання синхронних операцій, таких як створення і видалення таблиць.
- Використання методу scalar або scalars для вилучення об'єктів з результатів ORM-запитів.
Є й повністю асинхронні ORM, які спочатку розроблялися з підтримкою asyncio:
    Gino
    Pony.
    Tortoise
    Peewee

M7

M8
Особливості RDB:
    Atomicity - Атомарність гарантує, що жодна транзакція не буде зафіксована в системі частково. Будуть або виконані її підоперації, або не виконано жодної. Оскільки на практиці неможливо одночасно і атомарно виконати всю послідовність операцій всередині транзакції, вводиться поняття «відкочування» (rollback): якщо транзакцію не вдається повністю завершити, результати всіх її досі зроблених дій будуть скасовані і система повернеться до «зовнішнього вихідного» стану — зі сторони здаватиметься, що транзакції і не було. Звичайно, лічильники, індекси та інші внутрішні структури можуть змінитися, але якщо СУБД запрограмована без помилок, це не вплине на зовнішню її поведінку.
    Consistency - Узгодженість. Транзакція, що досягає свого нормального завершення (EOT - end of transaction, завершення транзакції) і, тим самим, фіксує свої результати, зберігає узгодженість бази даних. Інакше кажучи, кожна успішна транзакція за визначенням фіксує лише допустимі результати. Ця умова є необхідною для підтримки четвертої властивості.
    Isolation - Ізольованість. Під час виконання транзакції паралельні транзакції не повинні впливати на її результат. Ізольованість - вимога вартісна, тому в реальних БД існують режими, що не повністю ізолюють транзакцію (рівні ізольованості Repeatable Read і нижче).
    Durability - Довговічність. Незалежно від проблем на нижніх рівнях (наприклад, знеструмлення системи або збої в обладнанні) зміни, зроблені успішно завершеною транзакцією, повинні залишитися збереженими після повернення системи до роботи. Іншими словами, якщо користувач отримав підтвердження від системи, що транзакція виконана, він може бути впевнений, що зроблені ним зміни не будуть скасовані через будь-який збій.

CAP теорема
удь-яке сховище даних має три базові властивості:
    Узгодженість даних (Consistency). Тобто дані повинні бути повними та несуперечливими (включно і у всіх вузлах кластера).
    Доступність (Availability). Грубо кажучи, це швидкість відповіді сервера на наш запит (для запису або читання).
    Стійкість до поділу (Partition tolerance). Це означає, що у разі поділу системи на кілька частин, кожна з них, якщо вона доступна, повинна мати можливість працювати автономно, віддаючи коректний відгук та надаючи свої дані. Обрив зв'язків у кластері не повинен впливати на підсумкову роботу.

Реляційні бази даних реалізують CA комбінацію та є нестійкими до поділу.
NoSQL технології були народжені з метою вирішення проблеми стійкості до поділу, тобто ефективно працювати на кластерах. Реляційна модель не в змозі впоратися з цим завданням, оскільки була створена для інших цілей та інших умов. Вам не вдасться «просто відпиляти пару-трійку таблиць або спокійно їх поділити у сусідній кластер».
Сховища NoSQL за своєю природою можуть бути легко розділені на кластер через специфічну структуру зберігання даних.
Справжня сутність теореми CAP проявляється саме в умовах розподіленої системи. Очевидно, що створювати нестійкий до поділу кластер — позбавлено практичної користі. Тобто кластер апріорі повинен створюватися стійким до поділу. Розуміння цього факту дозволяє нам побачити теорему CAP у новому світлі: з узгодженості та доступності можна вибрати тільки щось одне — або використовувати розумний компроміс між цими двома пунктами (а не трьома, як можна подумати з оригінального визначення).
Друге завдання, яке намагаються вирішити ідеологи NoSQL технологій, - підвищення доступності, тобто отримувати швидку відповідь сервера.
Теорема CAP повідомляє нам, що із цих трьох компонентів ми можемо отримати лише два.

Типи NoSQL баз даних:
Key-value/Key-cache -  Memcached, структуровані дані Redis тощо.
Document Store -  MongoDB
Graph - Neo4j
Object Db - Realm
Wide Column Store -  Amazon DynamoDB, Apache Accumulo, Apache Cassandra

MongoDB є нереляційною базою даних типу NoSQL. База базується на моделі документів — об'єкти даних зберігаються у вигляді окремих документів у колекції.
Дані MongoDB групуються в колекції. Колекція — це збірник документів, які мають однакове призначення. Колекція подібна до таблиці в SQL базі даних, але відрізняється тим, що для колекції немає суворої схеми, і документи колекції можуть мати різну структуру.
Документ — це представлення елемента інформації у базі даних. Вони можуть складатися з підпорядкованих документів, і ця модель даних більше підходить для веб-застосунків. Максимальний розмір документа обмежений до 16 Мбайт.
У MongoDB у якості мови запитів використовується JavaScript та JSON-структури. Вибір мови запиту пояснюється тим, що MongoDB використовує JSON-формат для представлення документів та виведення результатів. Фізично JSON-структури зберігаються у бінарному BSON-форматі.
Первинний ключ. В SQL потрібно вказати будь-який унікальний стовпчик або комбінацію стовпчиків як первинний ключ. У MongoDB первинний ключ автоматично встановлюється у полі _id. Фактично змінна _id — це об'єкт типу ObjectId.
Він містить 12 байт, кожен із яких формується певним чином.
    4 - байтове значення (5f15996f), що позначає секунди, починаючи з останнього запису
    3 - байтове значення (bbde79), що позначає ідентифікатор машини
    2 - байтове значення (3a10), що позначає ідентифікатор процесу
    3 - байтовий лічильник (7af359), починаючи з випадкового значення

Існують деякі обмеження під час використання імен ключів:
-Символ $ не може бути першим символом в імені ключа
-Ім'я ключа не може містити символ крапки .
-Ім'я _id не рекомендується використовувати

У MongoDB у запитах можна використовувати умовні конструкції за допомогою операторів порівняння:
    $eq (дорівнює)
    $gt (більше ніж)
    $lt (менше ніж)
    $gte (більше або дорівнює)
    $lte (менше або дорівнює)

Іноді нам потрібна не вся інформація з документа, а, наприклад, певні поля, або, навпаки, деякі поля потрібно виключити
db.cats.find({age: {$lte: 3}, features: 'дає себе гладити'}, 
        {name: 0})
А можемо навпаки - залишити виведення лише двох необхідних полів
db.cats.find({age: {$lte: 3}, features: 'дає себе гладити'},
    {name: 1, age: 1},)
Варто зауважити, що поле _id виводиться завжди, якщо тільки примусово не заборонити його виведення _id: 0
Щоб обмежити вибірку, використовується функція limit. Наприклад, показати перші три документи в колекції
db.cats.find().limit(3)
Щоб пропустити кілька документів у вибірці, використовується функція skip. Наприклад, пропустити три документи у вибірці
db.cats.find().skip(3)
Сортування у вибірці виконується функцією sort, яка приймає об'єкт із полями для сортування і вони приймають значення: 1 за зростанням, -1 за спаданням
db.cats.find().sort({name: 1})
За допомогою функції count() можна отримати кількість елементів у колекції:
db.cats.count()
Модифікатори
Оператор $exists дозволяє витягти лише ті документи, в яких певний ключ присутній або відсутній.
db.cats.find({owners: {$exists: true}})
Оператор $type витягує лише ті документи, у яких певний ключ має значення певного типу, наприклад, рядок або число
db.cats.find({age: {$type: 'number'}})
Оператор $regex задає регулярний вираз, якому повинно відповідати значення поля.
db.cats.find({name: {$regex: 'L'}})
Логічні модифікатори
db.cats.find({$or: [{name: {$regex: 'L'}}, {age: {$lte: 3}}]})
Оператор логічного множення $and знаходить перетини вибірок
db.cats.find({$and: [{name: {$regex: 'L'}}, {age: {$lte: 3}}]})
Результат вибірки, що отримується за допомогою функції find, називається курсором. Курсори інкапсулюють у собі набори об'єктів, що отримуються з БД. Використовуючи синтаксис мови JavaScript та методи курсорів, ми можемо вивести отримані документи на екран та якось їх обробити.
const cursor = db.cats.find()
while (cursor.hasNext()) {
    obj = cursor.next()
    print(obj['name'])
}
Збереження документів можна виконати методом save. У новий документ як поле можна передати параметр _id. Якщо метод знаходить документ з таким значенням _id, то документ оновлюється. Якщо з таким _id немає документів, то документ вставляється.
db.cats.save({name: 'Bars', age: 3})

Детальніше налаштування під час оновлення пропонує функція update. Вона приймає три параметри:
    query: приймає запит на вибірку документа, який потрібно оновити
    objNew: надає документ з новою інформацією, який замістить старий під час оновлення
    options: визначає додаткові параметри під час оновлення документів. Може приймати два аргументи: upsert та multi.
Якщо параметр upsert має значення true, то MongoDB буде оновлювати документ, якщо він знайдений, і створювати новий, якщо такого документа немає. Якщо він має значення false, то MongoDB не буде створювати новий документ, якщо запит на вибірку не знайде жодного документа.
Параметр multi вказує, чи повинен оновлюватися перший елемент у вибірці (використовується за замовчуванням, якщо цей параметр не вказано) або повинні оновлюватися всі документи у вибірці.
Використання оператора $set приводить до того, що якщо документ не містить оновлюване поле, то воно створюється. Інакше буде зроблено заміну документа.
db.cats.update(
    {name: 'Tom'},
    {$set: {features: ['ходить в лоток', 'не дає себе гладити', 'сірий']}},
)
Вказавши значення multi:true, ми можемо оновити всі документи вибірки {multi:true}
Для видалення окремого ключа використовується оператор $unset:
db.cats.update({name: 'Tom'}, {$unset: {age: 1}})
Метод updateOne схожий на метод update за тим винятком, що він оновлює лише один документ. Якщо необхідно оновити всі документи, що відповідають деякому критерію, то застосовується метод updateMany
Для видалення документів у MongoDB передбачений метод remove:
Видалення всіх документів із зазначеним запитом
db.cats.remove({name: 'Tom'})
Якщо потрібно видалити тільки один документ
db.cats.remove({name: 'Tom'}, true)
Оператор $in визначає масив можливих виразів і шукає ті ключі, значення яких є в масиві:
db.cats.find({age: {$in: [2, 10]}})
Протилежним чином діє оператор $nin — він визначає масив можливих виразів і шукає ті ключі, значення яких відсутнє в цьому масиві
db.cats.find({age: {$nin: [2, 10]}})
Оператор $all схожий на $in: він також визначає масив можливих виразів, але вимагає, щоб документи мали увесь визначений набір виразів.
db.cats.find({features: {$all: ['ходить в лоток', 'дає себе гладити']}})
Оператор $size використовується для знаходження документів, в яких масиви мають кількість елементів, що дорівнює значенню $size.
db.cats.find({features: {$size: 3}})
Оператор $push додає значення в масив
db.cats.updateOne({name: 'Tom'}, {$push: {features: 'смердючий'}})
Якщо потрібно додати відразу кілька значень
db.cats.updateOne(
    {name: 'Tom'},
    {$push: {features: {$each: ['хропить', 'злий']}}},
)
Оператор $addToSet подібно до оператора $push додає об'єкти в масив. Відмінність полягає в тому, що $addToSet додає дані, якщо їх ще немає в масиві:
db.cats.update({name: 'Lama'}, {$addToSet: {features: 'божевільний'}})
Оператор $pop дозволяє видаляти елемент з масиву:
db.cats.update({name: 'Tom'}, {$pop: {features: 1}})
1 — кінець масиву, -1 — початок масиву
Оператор $pull видаляє за значенням
db.cats.update({name: 'Tom'}, {$pull: {features: 'сірий'}})
А якщо ми хочемо видалити не одне значення, а відразу декілька, тоді ми можемо застосувати оператор $pullAll:
db.cats.update(
    {name: 'Tom'},
    {$pullAll: {features: ['не дає себе гладити', 'смердючий', 'хропить']}},
)
Робота та підключення до Python
Встановимо драйвер Python під назвою "PyMongo". Існує багато драйверів, написаних спільнотою, але PyMongo є офіційним драйвером Python для MongoDB. Детальну документацію драйвера можна знайти тут.
Для створення документів MongoDB використовуються методи: insert_one - для вставлення одного документа і insert_many - для вставлення відразу кілька документів в колекцію.
Щоб отримати один документ, використовується метод find_one
Отримати декілька документів — метод find
Для оновлення документа можна використовувати метод update_one
Для видалення документа з колекції використовується метод delete_one
Усі доступні методи можна знайти в офіційній документації PyMongo https://pymongo.readthedocs.io/en/stable/

У цьому практичному прикладі ми створимо базу даних (БД) MongoDb з використанням ODM пакета MongoEngine.
Як ми вже сказали, як провідник даних ми будемо використовувати бібліотеку MongoEngine, яка надає ODM підхід до опису та управління об'єктами у БД MongoDB. По суті, MongoEngine є еквівалентом SQLAlchemy, але вже для MongoDB.
У цьому завданні ми використовуємо найпростіший спосіб приховування конфіденційних даних — це використання вбудованого модуля configparser та створення файлу config.ini
Тепер можна створити наші моделі даних. Абстракція MongoEngine заснована на класах, і всі створені моделі є класами.
Оголошення деяких властивостей у класі еквівалентно створенню структури даних для збереження даних. Ці класи прийнято зберігати у сценарії як модуль моделі застосунку.

База даних Redis дуже швидка, тому що використовує для своєї роботи тільки оперативну пам'ять (RAM) і зберігає там усі дані. Це, звичайно, може бути проблемою, якщо зникло живлення на сервері і потрібно відновити стан, але Redis вміє зберігати періодично свій стан на жорсткий диск, тому це зменшує ефект від втрати даних.
Redis має багато можливостей. Сценарій, що найчастіше використовується, — це кешування даних (кеш). Справа в роботі алгоритму TTL (Time To Live), який вбудований у Redis і дозволяє встановлювати для будь-якого запису час "життя" (TTL). Щойно проходить вказаний час, запис автоматично видаляється. Таким чином, розробник може використовувати Redis як тимчасове сховище даних для зменшення навантаження на реляційну базу даних (RDb) або сервер. При цьому не потрібно замислюватися над тим, як інвалідувати кеш.
Як ключі використовуються рядки. Якщо ви спробуєте використати якийсь інший тип даних як ключ, то Redis автоматично приведе цей тип до рядка. Про це потрібно пам'ятати, оскільки це неявне приведення типів може бути несподіваним.
Також важливо відзначити, що в Redis "з коробки" реалізовано паттерн проектування Publish/Subscribe.
Кешування
Тривалість зберігання даних у кеші можна встановити параметром default_ttl при виклику RedisLRU. Розмір кешу визначається параметром max_size.
Брокер повідомлень RabbitMQ
RabbitMQ - це програмне забезпечення для черги повідомлень, також відоме як брокер повідомлень або менеджер черг. Простіше кажучи — це програмне забезпечення, де визначаються черги, до яких підключаються застосунки для передачі повідомлення або повідомлень.
Найчастіше брокер повідомлень виступає посередником для різних послуг,наприклад, для веб-застосунків. Він може бути використаний для зниження навантажень на основний сервер застосунку, шляхом делегування завдань іншому сервісу. Як правило, такі завдання займають багато часу або ресурсів в основного сервера і віддають їх на обробку іншій стороні.
Базова архітектура черги повідомлень проста - є клієнтські застосунки, що називаються Producer (виробниками), які створюють повідомлення і доставляють їх в Broker (черга повідомлень). Інші застосунки, які називаються Consumer (споживачами), підключаються до черги та підписуються на обробку повідомлень. Повідомлення, поміщені у чергу, зберігаються доти, доки споживач не отримає їх.
Типи бірж:
direct: Повідомлення надсилається у черзі, ключ прив'язки яких точно збігається з ключем маршрутизації повідомлення.
fanout: Цей тип бірж направляє повідомлення на всі черги, прив'язані до нього.
topic: Біржа цього типу виконує зіставлення підстановних знаків між ключем маршрутизації та шаблоном маршрутизації, зазначеного у прив'язці.
headers: Біржа використовує атрибути заголовків повідомлень для маршрутизації.


video 11.03.24
0:50:00 - налаштування Docker compose
1:40:00 - regex Perl style
2:20:00 - $push, $each, $addToSet як $push ане не створює дублікат якщо значення вже є, $pop (-1) видалити з ПОЧАТКУ листа, (1) видалити з КІНЦЯ листа, $pull - як $pop але не по індексу а по значенню, $pullAll аналог конструкції $push...$each...
2:33:00 - odm як NoSQL аналог для urm в SQL

video 13.03.24
0:25:00 - Docker compose Redis+Mongo
1:09:00 - не можна кешувати змінні, яки змінюються іншими сервісами (бо нее обачиш зміни, звертаючись до закешованого старого значення)
1:18:00 - якщо в девелементі щось закешувалось в Redis - то зміни в коді закешованого не викликаються (бо закешовано старе). Щоб це виправити - треба або перевантажувати Redis (якщо не підключен зовнішній volume до контенера, де може зберегтися снепшот стану бази, який завантажиться знову) або міняти назву закешованої функції. clear_on_exit=True
1:40:00 - release candidate RC, -manaagement - WEB UI
1:45:00 - docker compose RabbitMQ

M9
Для початку імпортуємо потрібні пакети, зробимо запит за допомогою requests, у відповідь отримаємо HTML сторінки, що шукаємо. Далі перетворимо відповідь від сервера у формат lxml і передамо результат на обробку BeautifulSoup.
Lxml використовувати не обов'язково, BeautifulSoup вміє працювати і з "чистим" HTML, але бібліотека Lxml може трохи прискорити обробку.

Ще один хороший ресурс для вивчення скрапінгу - http://scrapingclub.com. Там є безліч посібників щодо використання іншого, просунутішого інструменту — Scrapy.

Навігація документом
Щоб отримати всі дочірні елементи першого тегу <p> на сторінці, використовуємо атрибут children
Щоб отримати батьківський елемент першого тегу <p> на сторінці, ми можемо використовувати властивість parent
Також можна використовувати методи find_parent і find_parents для пошуку батьківських елементів:
Ви можете отримати доступ до сусідніх елементів за допомогою атрибутів next_sibling та previous_sibling.
Пошук за CSS-селекторами
Метод select дозволяє шукати елементи на основі CSS-селекторів. Він приймає рядок із CSS-селектором і повертає всі елементи, що відповідають цьому селектору.
text = soup.select(".text")
Знайдемо всі елементи з ідентифікатором "header". Ідентифікатор - це спеціальний атрибут тегу id
header = soup.select("#header")
Комбіновані селектори
Комбіновані селектори шукають елементи, що відповідають кільком умовам.
Наприклад, знайдемо всі елементи <a> всередині тегу <div> з класом "container":
a = soup.select("div.container a")
Атрибути
Можна шукати елементи за значенням атрибутів. Знайдемо всі елементи, у яких атрибут href починається з "https://"
href = soup.select("[href^='https://']")
ctext = soup.select("[class*='text']")
Вибір між методом select і методом find залежить від ваших уподобань, рівня знань CSS-селекторів і вимог до пошуку елементів. Якщо вам зручніше працювати з CSS-селекторами та вам потрібно виконати складні запити, то select буде кращим вибором. Якщо вам потрібно просто знайти перший відповідний елемент, то find може бути зручнішим варіантом.

Фреймворк Scrapy
Для швидкого збору даних з однієї сторінки BeautifulSoup підходить на 100%. Але якщо вам потрібно зібрати дані на кількох сторінках, пройтися сторінками з пагінацією, то таке завдання значно ускладнює скрипт. Адже на кожній сторінці вам потрібно буде знайти ще й потрібне посилання, і потім зробити запит на це посилання. Крім того, менеджмент аутентифікації та/або авторизації знаходиться в зоні відповідальності розробника.
Останній недолік зв'язки BeautifulSoup та requests — це складність налаштування "ощадливого" скрапінгу. Коли потрібно зібрати дійсно багато даних із тисяч або десятків тисяч сторінок, ваш скрипт створить занадто велике та небажане навантаження на сервер. В результаті і, швидше за все, всі запити до цього сервера з вашої адреси будуть заблоковані. Щоб уникнути цього, потрібно робити запити з деяким інтервалом, немов імітуючи поведінку реального користувача. Реалізувати цей механізм може бути складним завданням.
Основні можливості Scrapy:
    асинхронність (значне зростання продуктивності);
    автоматична робота із заголовками запитів та куками;
    "ощадливий" скрапінг;
    імітація браузера зазначеної версії у заголовках запиту;
    інструменти для побудови складних багаторівневих схем обробки зібраних даних (очищення, валідація, форматування, збереження в базу даних тощо);
    власний інструмент тестування.
Scrapy використовує Spiders - набір автономних сканерів з певним набором інструкцій. За допомогою фреймворку легко розробити навіть великі проекти для скрапінгу так, щоб інші розробники могли використовувати цей код.
Scrapy надає оболонку веб-сканера Scrapy Shell, яку розробники можуть використовувати для перевірки своїх припущень щодо поведінки сайту. Щоб зайти в консоль Scrapy, виконайте у консолі: scrapy shell. Далі необхідно запустити парсер на сторінці за допомогою команди fetch в оболонці. Перед цим краще зберегти адресу сторінки, яку будемо парсити в змінну: url = 'https://quotes.toscrape.com/'.
Парсер повертає response (відповідь), яку можна подивитися за допомогою команди view(response). А сторінка відкриється в браузері за замовчуванням. За допомогою команди print(response.text) можна переглянути сирий HTML.
Scrapy використовує Tornado фреймворк і всі методи, які виконуються асинхронно, повинні бути генераторами, parse один з таких методів. Scrapy за типом результату, що повертається, визначає, що з ним потрібно зробити.
Щоб parse був генератором, потрібно повертати результати з нього, використовуючи оператор yield.

тут потрібно уточнити кілька моментів щод0о живого прикладу. Структура проекту збігається, але прибрано повернення результату у файл result.csv.
Оскільки розмір прикладу обмежений 1024 Мб і кожне виконання прикладу збільшуватиме файл result.csv. Друге, оскільки приклад може виконуватися тільки запуском файлу main.py в системі replit, то запуск краулера виконаний у вигляді скрипту, а не запуском команди scrapy crawl authors.

from scrapy.crawler import CrawlerProcess

class QuotesSpider(scrapy.Spider):
...
# run spider
process = CrawlerProcess()
process.crawl(QuotesSpider)
process.start()

Так, якби ми хотіли через скрипт main.py зберігати результат у файл result.csv, то просто додайте атрибут custom_settings до класу QuotesSpider:
class QuotesSpider(scrapy.Spider):
    name = 'authors'
    custom_settings = {"FEED_FORMAT": "csv", "FEED_URI": "result.csv"}
    ...


video 18.03.24
0:22:00 - Celetium - інструмент для парсенга клієнт-сайд рендеренге. На id не гарно орієнтуватися, краще по class і tag - id можуть генерувати рандомно (іноді навмисно)
1:10:00 - сервер може банити по сесії, яку зберігає і в якій є інфа по IP, router id, і т.п.

video 20.03.24
0:05:00 - BeautifulSoup це ліба, яких можна наімпортити багато і наш код буде контролювати потік виконання та визивати що нам треба, а Scrapy - це фреймворк, який використовується лише один, бо потік виконання контролюється фреймворком, який викликає наш код, який має бути написаний згідно правил фреймворка.
0:15:00 - пошук по сторінці
0:30:00 - пошук [text()='pattern'] по тексту всередені тегу, contains(@class 'humor')/@href по наявності тексту в атрибуті дістати href тегу, //*[contains(@class 'humor')] шукати будь-який тег з класом, де є humor
0:35:00 - [@class='tags and @itemprop='text'], [@class='tags or @itemprop='text'], //div[@class="tags"]/a[last()], //div[@class="tags"]/a[1] ~ first 
0:35:00 - //div[@class="tags"]/a/.. - перейти на батька, не важливо який він тег, //div[@class="tags"]/a/parent::div - перейти на батька, якщо він div
0:50:00 - //div[@class="tags"]/a[1]/following-sibling::a - всі наступні "a" на одному рівні
0:58:00 - як закривати модальні вікна на сайтах типу realpython.com
1:02:00 - robots.txt 













M10
Фреймворк відрізняється від поняття бібліотеки тим, що використовуючи бібліотеку (requests, NumPy тощо), програміст самостійно відповідає за потік застосунку. Тільки він вирішує, коли залучати до роботи сторонню функціональність. Фреймворк сам відповідає за потік. Він надає кілька місць для розміщення вашого коду, але викликати його або ні вирішує сам фреймворк (Django, FastAPI тощо).
Щоб глибоко розібратися в темі, ви можете написати свій веб-фреймворк, скориставшись подібною інструкцією. Але будьте готові до того, що написання власного веб-фреймворку — це дуже об'ємне і непросте завдання, яке потребує глибокого розуміння принципів роботи цілого набору технологій, що утворює сучасний Інтернет.
http://mattscodecave.com/posts/simple-python-framework-from-scratch.html
Ви звикли писати код, який робить щось корисне і те, як саме це "щось" робиться, знаходиться під вашим контролем. Використання фреймворку передбачає інший підхід. Ви повинні віддати контроль над виконанням запиту від користувача на сторону фреймворку та реалізувати потрібний функціонал, абстрагувавшись від деталей реалізації протоколів обміну.
Це важливий аспект і його порушення перетворює будь-який фреймворк з "друга" на "ворога". Замість того, щоб спростити розробку, спроба взяти контроль над потоком виконання застосунку ускладнить розробку, і вебфреймворк буде швидше заважати, ніж допомагати.
Архітектурно вебфреймворки можна розділити на:
- Model-view-controller (MVC).
- Багаторівнева організація.

«Модель-Представлення-Контролер» — схема поділу даних застосунку, інтерфейсу користувача і керуючої логіки на три окремих компоненти: модель, представлення і контролер таким чином, що модифікація кожного компонента може здійснюватися незалежно.
Модель (Model) надає дані та реагує на команди контролера, змінюючи свій стан.
Представлення (View) відповідає за відображення даних моделі користувача, реагуючи на зміни моделі.
Контролер (Controller) інтерпретує дії користувача, сповіщаючи модель про необхідність змін.
Такий підхід базується на тому, що формат зберігання даних не залежить від того, в якому форматі вони передаються користувачеві, а формат запиту не впливає на те, як цей запит обробляється. Така парадигма дозволяє для однієї і тієї самої бізнес-логіки зробити кілька варіантів використання (вебзастосунок, консольний застосунок, GUI-застосунок, чат-бот тощо). Для кожного такого варіанта буде своє представлення та контролер, але модель може бути спільна.
Багаторівнева організація
Поширені шари:
представлення, інтерфейс користувача відповідає за відображення даних у форматі, зручному клієнту, і перетворення клієнтських команд у формат шару застосунку;
застосунок відповідає за обробку користувальницьких команд і працює з шаром бізнес-логіки;
бізнес-логіка — шар предметної області, який не залежить від представлення та формату зберігання даних, реалізує набір ключових правил застосунку і є ядром застосунку;
шар даних відповідає за зберігання даних і надає дані у визначеному на рівні бізнес-логіки форматі.
Такий шаблон підходить для великих застосунків. Дозволяє максимально абстрагуватися від деталей реалізації зберігання даних, протоколів обміну командами та способу взаємодії з користувачем. Дуже гнучкий шаблон, але може бути надлишковим для невеликих простих застосунків.
Django - це один із найпопулярніших і найпотужніших фреймворків для створення web-застосунків.
Спочатку розробка Django велася для забезпечення зручнішої роботи з ресурсами новин, що досить сильно позначилося на архітектурі: фреймворк надає ряд засобів, які допомагають у швидкій розробці веб-сайтів інформаційного характеру.
​Фреймворк Django справляється з великою кількістю завдань та підвищеними навантаженнями. Його застосовують для створення:
CRM-систем (Customer relationship management).
CMS (Content management system).
Комунікаційних платформ.
Сервісів бронювання номерів.
Платформ управління документообігом.
Також Django підходить для створення алгоритмічних генераторів, платформ для електронних розсилок, систем верифікації, систем фільтрації з динамічними правилами та складними параметрами, платформ для аналізу даних та складних обчислень, машинного навчання.

CAUTION
Зверніть увагу, що Django не гарантує зворотну сумісність між версіями. Застосунок, написаний для Django 3.0, може не працювати з версією 3.2.

Зверніть увагу, ім'я проекту не повинно перетинатися з іменами, які використовує Django, наприклад django, test тощо.
manage.py — відповідає за взаємодію із застосунком через термінал;
mysite/settings.py — містить налаштування застосунку;
mysite/urls.py — модуль, де описані всі URL-адреси і як вони повинні вирішуватися, відповідає за роутинг застосунку;
mysite/asgi.py та mysite/wsgi.py — є точками входу для ASGI та WSGI веб -серверів для роботи з застосунком Django.
Щоб запустити локально сервер для розробки, достатньо виконати команду в консолі python manage.py runserver, знаходячись всередині папки проекту mysite.
Цей сервер не підходить для використання в реальних застосунках, але дуже корисний на етапі розробки.

Для стилізації шаблонів ми використовуємо популярний фреймворк Bootstrap https://getbootstrap.com/
